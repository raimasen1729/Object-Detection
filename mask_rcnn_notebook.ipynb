{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIS 680 - Final Project - Mask R-CNN\n",
        "\n",
        "#### Raima Sen\n",
        "#### Shreyas Ramesh\n",
        "\n",
        "This project aims to develop a pixel wise framework that combines semantic segmentation and object detection to perform instance segmentation. The primary goal is to extend Faster RCNN which outputs the class label and bounding box of candidate objects. This\n",
        "extension is brought about with an additional output : an object mask for each candidate object.\n",
        "\n",
        "References:\n",
        "\n",
        "[1] Umer Farooq. From R-CNN to mask R-CNN. Feb. 2018. URL: https://medium.com/@umerfarooq_26378/from-r-cnn-to-mask-r-cnn-d6367b196cfd\n",
        "\n",
        "[2] Ross Girshick. Fast R-CNN. Sept. 2015. URL: https://arxiv.org/abs/1504.08083.\n",
        "\n",
        "[3] Ross Girshick et al. Rich feature hierarchies for accurate object detection and semantic segmentation. Oct. 2014. URL: https://arxiv.org/abs/1311.2524.\n",
        "\n",
        "[4] Kaiming He et al. Mask R-CNN. Jan. 2018. URL: https://arxiv.org/abs/1703.06870.\n",
        "\n",
        "[5] Chuan Liu, Yi Gao, and Jiancheng Lv. “Dynamic Normalization”. In: CoRR abs/2101.06073 (2021). arXiv: 2101.06073. URL: https://arxiv.org/abs/2101.06073.\n",
        "\n",
        "[6] Shaoqing Ren et al. Faster R-CNN: Towards real-time object detection with region proposal networks. Jan. 2016. URL: https://arxiv.org/abs/1506.01497.\n",
        "\n"
      ],
      "metadata": {
        "id": "gGadTtJTitJC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZiiM76ZR5n"
      },
      "source": [
        "# Part 1 : Initialize \n",
        "- Imports \\\\\n",
        "- Mount Drive \\\\\n",
        "- Set Cuda \\\\\n",
        "- Define global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knae4rUPR7Lh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.ops import focal_loss\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.functional import pad, normalize, resize\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "from  matplotlib.patches import Rectangle as rec\n",
        "from torch import nn, Tensor\n",
        "from torchvision.ops import box_iou\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from sklearn.metrics import auc\n",
        "import pdb\n",
        "!pip install pytorch_lightning &> /dev/null\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.loggers as pl_loggers\n",
        "import pytorch_lightning.callbacks as pl_callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LGe5xqoZbHP",
        "outputId": "a76e688f-ef48-4624-895d-479a82cdf2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qxQcgoAZdLw",
        "outputId": "251ab4a6-a279-4e7b-cf59-ec278f9cd355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxiVs_cYZfVE"
      },
      "outputs": [],
      "source": [
        "h_img = 800\n",
        "w_img = 1088\n",
        "debug = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHGxdzb-m8PM"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "def MultiApply(func, *args, **kwargs):\n",
        "    pfunc = partial(func, **kwargs) if kwargs else func\n",
        "    map_results = map(pfunc, *args)\n",
        "  \n",
        "    return tuple(map(list, zip(*map_results)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-ZO-inXZr9-"
      },
      "source": [
        "# Part 2 : Loading the Data\n",
        "- class : BuildDataset\n",
        "- class : BuildDataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DijR0BlbbgHJ"
      },
      "outputs": [],
      "source": [
        "class BuildDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.images_path = os.path.join(path[0])  \n",
        "        self.masks_path = os.path.join(path[1])    \n",
        "        self.labels_path = os.path.join(path[2])\n",
        "        self.bboxes_path = os.path.join(path[3])\n",
        "        self.images = np.array(h5py.File(self.images_path, 'r')['data'])    #transformation needed : 0-1, resize, normalize, pad\n",
        "        self.masks = np.array(h5py.File(self.masks_path, 'r')['data'])      #transformation needed : resize, pad\n",
        "        self.bboxes = np.load(self.bboxes_path, allow_pickle=True, encoding='latin1')    #transformation needed : scaling and add padding\n",
        "        self.labels = np.load(self.labels_path, allow_pickle=True, encoding='latin1')\n",
        "        self.indexes = []\n",
        "        temp = 0\n",
        "        for i in range(len(self.images)):\n",
        "          self.indexes.append(temp)\n",
        "          temp += self.labels[i].shape[0]\n",
        "          \n",
        "        self.indexes = np.array(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #example : images[5] has 2 objects in it; so there will be 2 labels, 2 corresponding masks and 2 corresponding bounding boxes\n",
        "        image = torch.from_numpy(self.images[index].astype('float32')).to(device)                                               #previously was uint32\n",
        "        labels = torch.from_numpy(self.labels[index]).to(device)                                                                #int64\n",
        "        n_obj = labels.shape[0]                  #number of objects present in the image\n",
        "        masks = torch.from_numpy(self.masks[self.indexes[index] : self.indexes[index] + n_obj].astype('float32')).to(device)    #previously was uint32\n",
        "        bboxes = torch.from_numpy(self.bboxes[index]).to(device)                                                                #float32  \n",
        "        transed_img, transed_masks, transed_bboxes = self.pre_process_single(image, masks, bboxes)\n",
        "        return transed_img, labels, transed_masks, transed_bboxes, index\n",
        "\n",
        "    #the 3 transformations below are defined for single image and its corresponding (sometimes multiple) masks and bounding boxes\n",
        "    def pre_process_single(self, image, masks, bboxes):   \n",
        "        #img transformation : 0-1, resize, normalize, pad\n",
        "        img = pad(normalize(resize(torch.div(image, 255), size=(800,1066)), (0.485,0.456,0.406), (0.229,0.224,0.225)), [11,0])\n",
        "        #mask transformation : resize, pad\n",
        "        new_masks = pad(resize(masks, size=(800,1066)), [11,0])\n",
        "        #bounding boxes transformation : scaling and pad\n",
        "        old_h = 300\n",
        "        old_w = 400\n",
        "        new_h = 800\n",
        "        new_w = 1066\n",
        "        padding = 22\n",
        "        scale_x = new_w/old_w\n",
        "        scale_y = new_h/old_h\n",
        "        new_bboxes = torch.zeros_like(bboxes)\n",
        "        new_bboxes[:,0] = scale_x * bboxes[:,0] - padding/2\n",
        "        new_bboxes[:,1] = scale_y * bboxes[:,1]\n",
        "        new_bboxes[:,2] = scale_x * bboxes[:,2] + padding/2\n",
        "        new_bboxes[:,3] = scale_y * bboxes[:,3]\n",
        "\n",
        "        return img, new_masks, new_bboxes\n",
        "       \n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPU-oaJHrMMn"
      },
      "outputs": [],
      "source": [
        "class BuildDataLoader(torch.utils.data.DataLoader):\n",
        "    def __init__(self, dataset, batch_size, shuffle, num_workers):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def collect_fn(self, batch):\n",
        "        images, labels, masks, bounding_boxes, index = list(zip(*batch))\n",
        "        return torch.stack(images), labels, masks, bounding_boxes, index\n",
        "\n",
        "    def loader(self):\n",
        "        return DataLoader(self.dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=self.shuffle,\n",
        "                          num_workers=self.num_workers,\n",
        "                          collate_fn=self.collect_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzoe00BdJ5lS"
      },
      "source": [
        "# Part 3 : Instantiate BuildDataset and BuildDataLoader classes\n",
        "- run only before training to prevent a lot of memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmsUrDY4KLd5",
        "outputId": "6b581040-c889-4c9f-db06-84e536f31491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 2\n"
          ]
        }
      ],
      "source": [
        "imgs_path = '/content/drive/MyDrive/CIS680/HW4/data/images.h5'\n",
        "masks_path = '/content/drive/MyDrive/CIS680/HW4/data/masks.h5'\n",
        "labels_path = '/content/drive/MyDrive/CIS680/HW4/data/labels.npy'\n",
        "bboxes_path = '/content/drive/MyDrive/CIS680/HW4/data/bboxes.npy'\n",
        "\n",
        "paths = [imgs_path, masks_path, labels_path, bboxes_path]\n",
        "dataset = BuildDataset(paths)\n",
        "\n",
        "subset = []\n",
        "# change the number of samples as needed. taking 10 now for sanity checking\n",
        "for i in range(200):\n",
        "   subset.append(dataset[i])\n",
        "\n",
        "full_size = len(subset)\n",
        "train_size = int(full_size * 0.9)\n",
        "test_size = full_size - train_size\n",
        "\n",
        "torch.random.manual_seed(1)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(subset, [train_size, test_size])\n",
        "\n",
        "batch_size = 2\n",
        "print(\"batch size:\", batch_size)\n",
        "test_build_loader = BuildDataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = test_build_loader.loader()\n",
        "train_build_loader = BuildDataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "train_loader = train_build_loader.loader()\n",
        "full_build_loader = BuildDataLoader(subset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "full_loader = full_build_loader.loader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muzwSbYbKfMR"
      },
      "outputs": [],
      "source": [
        "del train_dataset\n",
        "del test_dataset\n",
        "del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkpytJ7Pe1P9"
      },
      "source": [
        "# Part 4 : RPN Head\n",
        "- Backbone is Resnet50 FPN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtxTpIlFJnVH"
      },
      "source": [
        "## class RPNHead definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUkr9qh6e__i"
      },
      "outputs": [],
      "source": [
        "class RPNHead(nn.Module):\n",
        "    def __init__(self, num_anchors=3, in_channels=256, device='cuda',\n",
        "                 anchors_param=dict(ratio=[[1, 0.5, 2], [1, 0.5, 2], [1, 0.5, 2], [1, 0.5, 2], [1, 0.5, 2]],\n",
        "                                    scale=[32, 64, 128, 256, 512],\n",
        "                                    grid_size=[(200, 272), (100, 136), (50, 68), (25, 34), (13, 17)],\n",
        "                                    stride=[4, 8, 16, 32, 64])\n",
        "                 ):\n",
        "        super(RPNHead, self).__init__()\n",
        "        self.device=device\n",
        "        self.num_anchors = num_anchors\n",
        "        self.in_channels = in_channels\n",
        "        self.ratio = anchors_param[\"ratio\"]\n",
        "        self.scale = anchors_param[\"scale\"]\n",
        "        self.grid_size = anchors_param[\"grid_size\"]\n",
        "        self.stride = anchors_param[\"stride\"]\n",
        "        self.h_img = h_img\n",
        "        self.w_img = w_img\n",
        "        #Defining resnet50 FPN backbone\n",
        "        pretrained_model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True)\n",
        "        self.backbone = pretrained_model.backbone\n",
        "        self.intermediate = nn.Sequential(\n",
        "                                          nn.Conv2d(in_channels = self.in_channels, out_channels=256, kernel_size=3, padding = 'same'),\n",
        "                                          nn.BatchNorm2d(num_features=256),\n",
        "                                          nn.ReLU()\n",
        "                                          )\n",
        "        self.classifier = nn.Sequential(\n",
        "                                        nn.Conv2d(in_channels = self.in_channels, out_channels = 1*self.num_anchors, kernel_size=1, padding = 'same'),\n",
        "                                        nn.Sigmoid()\n",
        "                                        )\n",
        "        self.Regressor = nn.Sequential(\n",
        "                                        nn.Conv2d(in_channels = self.in_channels, out_channels = 4*1*self.num_anchors, kernel_size=1, padding = 'same') \n",
        "                                        )\n",
        "        self.anchors = self.create_anchors(self.ratio, self.scale, self.grid_size, self.stride)\n",
        "\n",
        "    '''\n",
        "    Forward each level of the FPN output through the intermediate layer and the RPN heads\n",
        "    Input:\n",
        "          X: list:len(FPN){(bz,256,grid_size[0],grid_size[1])}\n",
        "    Ouput:\n",
        "          logits: list:len(FPN){(bz,1*num_anchors,grid_size[0],grid_size[1])}\n",
        "          bbox_regs: list:len(FPN){(bz,4*num_anchors, grid_size[0],grid_size[1])}\n",
        "    '''\n",
        "    def forward(self, images):\n",
        "        logits = []\n",
        "        bbox_regs = []\n",
        "        feature_pyramid = [v.detach() for v in self.backbone(images).values()]\n",
        "        for i in range(len(feature_pyramid)):\n",
        "          logit, bbox_reg = self.forward_single(feature_pyramid[i])\n",
        "\n",
        "          logits.append(logit)\n",
        "          bbox_regs.append(bbox_reg)\n",
        "\n",
        "        return logits, bbox_regs\n",
        "        \n",
        "    '''\n",
        "    Forward a single level of the FPN output through the intermediate layer and the RPN heads\n",
        "    Input:\n",
        "          feature: (bz,256,grid_size[0],grid_size[1])}\n",
        "    Ouput:\n",
        "          logit: (bz,1*num_anchors,grid_size[0],grid_size[1])\n",
        "          bbox_regs: (bz,4*num_anchors, grid_size[0],grid_size[1])\n",
        "    '''\n",
        "    def forward_single(self, feature):\n",
        "        X = self.intermediate(feature)\n",
        "        logit = self.classifier(X)\n",
        "        bbox_reg = self.Regressor(X)\n",
        "        return logit, bbox_reg\n",
        "\n",
        "    '''\n",
        "    This function creates the anchor boxes for all FPN level\n",
        "    Input:\n",
        "          aspect_ratio: list:len(FPN){list:len(number_of_aspect_ratios)}\n",
        "          scale:        list:len(FPN)\n",
        "          grid_size:    list:len(FPN){tuple:len(2)}\n",
        "          stride:        list:len(FPN)\n",
        "    Output:\n",
        "          anchors_list: list:len(FPN){(num_anchors,grid_size[0],grid_size[1],4)}\n",
        "    '''\n",
        "    def create_anchors(self, aspect_ratio, scale, grid_size, stride):\n",
        "        anchors_list = []\n",
        "        for i in range(len(aspect_ratio)):\n",
        "          anchors = self.create_anchors_single(aspect_ratio[i], scale[i], grid_size[i], stride[i])\n",
        "          anchors_list.append(anchors)\n",
        "        return anchors_list\n",
        "\n",
        "    '''\n",
        "    This function creates the anchor boxes for one FPN level\n",
        "    Input:\n",
        "          aspect_ratio: list:len(number_of_aspect_ratios)\n",
        "          scale: scalar\n",
        "          grid_size: tuple:len(2)\n",
        "          stride: scalar\n",
        "    Output:\n",
        "          anchors: (num_anchors,grid_size[0],grid_size[1],4)\n",
        "    '''\n",
        "    def create_anchors_single(self, aspect_ratio, scale, grid_size, stride):\n",
        "        x = torch.arange(start=int(stride/2) , end=int(self.w_img + stride / 2), step=stride)\n",
        "        y = torch.arange(start=int(stride/2) , end=int(self.h_img + stride / 2), step=stride)\n",
        "        xx,yy  = torch.meshgrid(x,y)\n",
        "        anchors = []\n",
        "        for aspect in aspect_ratio:\n",
        "          h = scale / math.sqrt(aspect)\n",
        "          w = scale * math.sqrt(aspect)\n",
        "          anchor = torch.zeros((grid_size[0], grid_size[1], 4)) # (x, y, w, h) - Sy, Sx, 4\n",
        "          anchor[:, :, 0] = xx.T\n",
        "          anchor[:, : ,1] = yy.T\n",
        "          anchor[:, :, 2] = w\n",
        "          anchor[:, :, 3] = h\n",
        "          anchors.append(anchor)\n",
        "        anchors = torch.stack(anchors)       \n",
        "        #anchors = anchors.reshape((-1,4))\n",
        "        return anchors\n",
        "\n",
        "    def get_anchors(self):\n",
        "        return self.anchors\n",
        "\n",
        "    '''\n",
        "    This function creates the ground truth for a batch of images\n",
        "    Input:\n",
        "          bboxes_list: list: len(bz){(number_of_boxes,4)}\n",
        "    Ouput:\n",
        "          ground: list:len(FPN){(bz,num_anchors,grid_size[0],grid_size[1])}\n",
        "          ground_coord: list:len(FPN){(bz,4*num_anchors,grid_size[0],grid_size[1])}\n",
        "    '''\n",
        "    #The below note is done\n",
        "    #abhi ke liye iska shape hai len(bz), len(FPN), ...\n",
        "    #karna padega as len(FPN), (bz, num_anchors...)\n",
        "    def create_batch_truth(self, bboxes_list):\n",
        "        ground_list=[]\n",
        "        ground_coord_list=[]\n",
        "        for idx in range(len(bboxes_list)):\n",
        "          ground_class, ground_crd = self.create_ground_truth(bboxes_list[idx], self.grid_size, self.anchors)       \n",
        "          ground_list.append(ground_class)\n",
        "          ground_coord_list.append(ground_crd)    \n",
        "        ground = []\n",
        "        ground_coord = []\n",
        "        for lvl in range(len(ground_list[0])):  #iterate over FPN levels\n",
        "          temp_list1 = []\n",
        "          temp_list2 = []\n",
        "          for i in range(len(ground_list)):     #iterate over batch_size\n",
        "            temp_list1.append(ground_list[i][lvl])\n",
        "            temp_list2.append(ground_coord_list[i][lvl])         \n",
        "          ground.append(torch.stack(temp_list1))\n",
        "          ground_coord.append(torch.stack(temp_list2))\n",
        "        return ground, ground_coord\n",
        "\n",
        "    '''\n",
        "    This function create the ground truth for ONE IMAGE for all the FPN levels\n",
        "    Input:\n",
        "          bboxes:      (n_boxes,4)\n",
        "          grid_size:   list:len(FPN){tuple:len(2)}\n",
        "          anchor_list: list:len(FPN){(num_anchors,grid_size[0],grid_size[1],4)}\n",
        "    Output:\n",
        "          ground_clas: list:len(FPN){(num_anchors,grid_size[0],grid_size[1])}\n",
        "          ground_coord: list:len(FPN){(4*num_anchors,grid_size[0],grid_size[1])}\n",
        "    '''\n",
        "    def create_ground_truth(self, bboxes, grid_sizes, anchors_list):\n",
        "        #note : bboxes are the ground truth in x1,y1,x2,y2 format\n",
        "        #note : prefer doing calc in flattened mode\n",
        "        bboxes = bboxes.to(device)\n",
        "        fpn_levels = len(anchors_list)\n",
        "        ground_clas = []\n",
        "        ground_coord = []\n",
        "        for lvl in range(fpn_levels):\n",
        "          class_ls = []\n",
        "          coord_ls = []\n",
        "          anchors = anchors_list[lvl].to(device)\n",
        "          for a in range(self.num_anchors):\n",
        "            flat_anchors = anchors[a].permute(2,0,1).flatten(start_dim=1,end_dim=-1).T\n",
        "            # pdb.set_trace()\n",
        "            x1=flat_anchors[:,0]-flat_anchors[:,2]/2.0\n",
        "            y1=flat_anchors[:,1]-flat_anchors[:,3]/2.0\n",
        "            x2=flat_anchors[:,0]+flat_anchors[:,2]/2.0\n",
        "            y2=flat_anchors[:,1]+flat_anchors[:,3]/2.0\n",
        "            # convert flattened anchors from x,y,w,h to x1,y1,x2,y2 format\n",
        "            formated_flat_anchors = torch.hstack((x1.reshape(-1,1),y1.reshape(-1,1),x2.reshape(-1,1),y2.reshape(-1,1)))\n",
        "\n",
        "            ground_crd = torch.zeros((self.grid_size[lvl][0]*self.grid_size[lvl][1],4)).to(device)\n",
        "            ground_cls = -1*torch.ones(self.grid_size[lvl][0]*self.grid_size[lvl][1]).to(device)\n",
        "\n",
        "            # calculating IOU with all anchors (even the ones that are out of the image boundaries)\n",
        "            iou = box_iou(formated_flat_anchors,bboxes)\n",
        "\n",
        "            # convert bboxes from x1,y1,x2,y2 to x,y,w,h format so that we can encode them\n",
        "            x = (bboxes[:,0]+bboxes[:,2])/2.0\n",
        "            y = (bboxes[:,1]+bboxes[:,3])/2.0\n",
        "            w = (bboxes[:,2]-bboxes[:,0])\n",
        "            h = (bboxes[:,3]-bboxes[:,1])\n",
        "            formated_bboxes = torch.hstack((x.reshape(-1,1),y.reshape(-1,1),w.reshape(-1,1),h.reshape(-1,1)))\n",
        "\n",
        "            # FINDING POSITIVE LABELS and corresponding ground truth bboxes\n",
        "            pos_idx_mask = torch.max(iou,dim=1)[0]>=0.7\n",
        "            pos_idx = pos_idx_mask.nonzero().flatten()\n",
        "            ground_cls[pos_idx] = 1\n",
        "            # finding corresponding bbox and anchor info\n",
        "            bbox_index = torch.max(iou,dim=1)[1]\n",
        "            bbox_label = bbox_index[pos_idx]\n",
        "            anchor_data = flat_anchors[pos_idx]\n",
        "            xa = anchor_data[:,0]\n",
        "            ya = anchor_data[:,1]\n",
        "            wa = anchor_data[:,2]\n",
        "            ha = anchor_data[:,3]\n",
        "            # encoding to tx, ty, tw, th\n",
        "            ground_crd[pos_idx,0]= (formated_bboxes[bbox_label,0]-xa)/wa\n",
        "            ground_crd[pos_idx,1]= (formated_bboxes[bbox_label,1]-ya)/ha\n",
        "            ground_crd[pos_idx,2]= torch.log(formated_bboxes[bbox_label,2]/wa)\n",
        "            ground_crd[pos_idx,3]= torch.log(formated_bboxes[bbox_label,3]/ha)\n",
        "\n",
        "            # FINDING IN BETWEEN LABELS and corresponding ground truth bboxes\n",
        "            in_between = torch.logical_and(torch.max(iou,dim=1)[0]<0.7,torch.max(iou,dim=1)[0]>0.4)\n",
        "            in_between_idx = in_between.nonzero().flatten()\n",
        "            ground_cls[in_between_idx] = 1\n",
        "            # finding corresponding bbox and anchor info\n",
        "            bbox_label = bbox_index[in_between_idx]\n",
        "            anchor_data = flat_anchors[in_between_idx]\n",
        "            xa = anchor_data[:,0]\n",
        "            ya = anchor_data[:,1]\n",
        "            wa = anchor_data[:,2]\n",
        "            ha = anchor_data[:,3]\n",
        "            # encoding to tx, ty, tw, th\n",
        "            ground_crd[in_between_idx,0]= (formated_bboxes[bbox_label,0]-xa)/wa\n",
        "            ground_crd[in_between_idx,1]= (formated_bboxes[bbox_label,1]-ya)/ha\n",
        "            ground_crd[in_between_idx,2]= torch.log(formated_bboxes[bbox_label,2]/wa)\n",
        "            ground_crd[in_between_idx,3]= torch.log(formated_bboxes[bbox_label,3]/ha)\n",
        "\n",
        "            # FINDING NEGATIVE LABELS (they don't have any ground truth boxes attached)\n",
        "            ground_cls[torch.all(iou<=0.3,dim=1)]=0\n",
        "\n",
        "            #ELIMINATING CROSS BOUNDARY ANCHORS\n",
        "            cross_bound_l = torch.logical_or(x1<=0,y1<=0)\n",
        "            cross_bound_h = torch.logical_or(x2>=1088.0,y2>=800)\n",
        "            ground_cls[cross_bound_l]=-1\n",
        "            ground_cls[cross_bound_h]=-1\n",
        "\n",
        "            ground_cls = ground_cls.unsqueeze(dim=0).reshape(1,self.grid_size[lvl][0],self.grid_size[lvl][1])\n",
        "            ground_crd = ground_crd.T.reshape(4,self.grid_size[lvl][0],self.grid_size[lvl][1])\n",
        "\n",
        "            class_ls.append(ground_cls)\n",
        "            coord_ls.append(ground_crd)\n",
        "\n",
        "          ground_clas.append(torch.cat(class_ls))\n",
        "          ground_coord.append(torch.cat(coord_ls))\n",
        "          \n",
        "        return ground_clas, ground_coord\n",
        "\n",
        "    '''\n",
        "    Compute the loss of the classifier for a level but for all images\n",
        "    '''\n",
        "    def loss_class(self, pos_class_gt, pos_class_pred, neg_class_gt, neg_class_pred, pos_anchors, neg_anchors):\n",
        "        skip_flag_pos = False\n",
        "        skip_flag_neg = False\n",
        "        if (pos_anchors.shape[0]==0):\n",
        "          skip_flag_pos = True\n",
        "        if (neg_anchors.shape[0]==0):\n",
        "          skip_flag_neg = True\n",
        "        criterion = torch.nn.BCELoss()\n",
        "        #seperately calc postiive class and negative class loss:\n",
        "        neg_class_loss = criterion(neg_class_pred+1e-10,neg_class_gt) if skip_flag_neg == False else torch.tensor(0)\n",
        "        pos_class_loss = criterion(pos_class_pred+1e-10, pos_class_gt) if skip_flag_pos == False else torch.tensor(0)\n",
        "        loss_c = neg_class_loss + pos_class_loss\n",
        "        #print(loss_c)\n",
        "        return loss_c\n",
        "   \n",
        "    '''\n",
        "    Compute the loss of the regressor for a level but over all images\n",
        "    '''\n",
        "    def loss_reg(self, pos_regr_gt, pos_regr_pred):\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss_r = sum([criterion(pos_regr_gt[i], pos_regr_pred[i]) for i in range(4)])\n",
        "        #print(loss_r)\n",
        "        return loss_r\n",
        "\n",
        "    '''\n",
        "    Compute the total loss\n",
        "    Input:\n",
        "          clas_out_list: list:len(FPN){(bz,1*num_anchors,grid_size[0],grid_size[1])}\n",
        "          regr_out_list: list:len(FPN){(bz,4*num_anchors,grid_size[0],grid_size[1])}\n",
        "          targ_clas_list: list:len(FPN){(bz,1*num_anchors,grid_size[0],grid_size[1])}\n",
        "          targ_regr_list: list:len(FPN){(bz,4*num_anchors,grid_size[0],grid_size[1])}\n",
        "          l: weighting lambda between the two losses\n",
        "          effective_batch: the number of anchors in the effective batch (M in the handout)\n",
        "    '''\n",
        "    def compute_loss(self, class_out_list, regr_out_list, targ_class_list, targ_regr_list, l=1, effective_batch=50):\n",
        "        fpn_levels = len(class_out_list)\n",
        "        batch_size = len(class_out_list[0])\n",
        "        loss_c = 0\n",
        "        loss_r = 0\n",
        "        for lvl in range(fpn_levels):\n",
        "          #step 1: flatten the class_out and targ_class\n",
        "          targ_class = targ_class_list[lvl].reshape(-1)\n",
        "          class_out = class_out_list[lvl].reshape(-1)\n",
        "          #step 2: create mini batch\n",
        "          pos_anchors = (targ_class==1).nonzero()\n",
        "          neg_anchors = (targ_class==0).nonzero()\n",
        "          pos_size = int( min(pos_anchors.shape[0], effective_batch/2))\n",
        "          neg_size = int(effective_batch - pos_size)\n",
        "          pos_anchors = pos_anchors[torch.randperm(pos_anchors.shape[0]), :]\n",
        "          pos_anchors = pos_anchors[:pos_size, :]\n",
        "          neg_anchors = neg_anchors[torch.randperm(neg_anchors.shape[0]), :]\n",
        "          neg_anchors = neg_anchors[:neg_size, :]\n",
        "          #step 3: assign positive and negative ground truths and predictions for the classifier\n",
        "          pos_class_gt = targ_class[pos_anchors]\n",
        "          pos_class_pred = class_out[pos_anchors]\n",
        "          neg_class_gt = targ_class[neg_anchors]\n",
        "          neg_class_pred = class_out[neg_anchors]\n",
        "\n",
        "          #step 4: calculate classifier loss for both positive and negative ground truth labels and sum over all FPN levels \n",
        "          loss_c += self.loss_class(pos_class_gt, pos_class_pred, neg_class_gt, neg_class_pred, pos_anchors, neg_anchors)\n",
        "\n",
        "          # step 5: calculate regressor loss for only positive ground truth labels\n",
        "          #reshape the regressor outputs and targets from (bs, 4*num_anchors, Sy, Sx) to (4*num_anchors, bs, Sy, Sx)\n",
        "          targ_regr = targ_regr_list[lvl].permute(1,0,2,3)\n",
        "          regr_out = regr_out_list[lvl].permute(1,0,2,3)\n",
        "          #flatten the regressor output and targets to (4, bs*Sy*Sx)\n",
        "          targ_regr = targ_regr.reshape(4,-1)\n",
        "          regr_out = regr_out.reshape(4,-1)\n",
        "          #get only postive ground truth labels for the regressor\n",
        "          pos_regr_gt = targ_regr[:, pos_anchors]\n",
        "          pos_regr_pred = regr_out[:, pos_anchors]\n",
        "          #extremely kaam chalau method below to eliminate the fpn levels where no bounding boxes are predicted\n",
        "          if len(pos_regr_pred[0])!=0 and len(pos_regr_gt[0])!=0:\n",
        "            #call regressor loss and sum over all FPN levels where the list is not empty\n",
        "            loss_r += self.loss_reg(pos_regr_gt, pos_regr_pred)\n",
        "\n",
        "        #here i am taking the average over the batch size\n",
        "        loss_classifier = loss_c/batch_size\n",
        "        loss_regressor = loss_r/batch_size\n",
        "        loss = loss_classifier + l * loss_regressor\n",
        "\n",
        "        return loss, loss_classifier, loss_regressor\n",
        "    \n",
        "    '''\n",
        "    Input  : clas:(top_k_boxes)\n",
        "             prebox:(top_k_boxes,4)\n",
        "    Output  :\n",
        "    nms_clas : (remaining_boxes_after_nms)\n",
        "    nms_prebox : (remaining_boxes_after_nms, 4)\n",
        "    '''\n",
        "    def NMS(self, clas, prebox, thresh):\n",
        "        bbox_sorted=deepcopy(prebox.permute(1,0).detach().cpu())\n",
        "        clas_sorted=deepcopy(clas.detach().cpu())\n",
        "        nms_prebox=[]\n",
        "        nms_clas=[]\n",
        "        bbox_sorted=list(bbox_sorted.numpy())\n",
        "        clas_sorted=list(clas_sorted.numpy())\n",
        "\n",
        "        while len(bbox_sorted)!=0:\n",
        "          curr_bbox=bbox_sorted[0]\n",
        "          curr_conf=clas_sorted[0]\n",
        "          bbox_sorted.remove(curr_bbox)\n",
        "          clas_sorted.remove(curr_conf)\n",
        "          nms_prebox.append(curr_bbox)\n",
        "          nms_clas.append(curr_conf)\n",
        "          for id,diff_boxes in enumerate(bbox_sorted):\n",
        "            if(box_iou(torch.from_numpy(curr_bbox).reshape(1,4),torch.from_numpy(diff_boxes).reshape(1,4))[0]>0.5):\n",
        "              del bbox_sorted[id]\n",
        "              del clas_sorted[id]\n",
        "\n",
        "        return nms_clas, nms_prebox\n",
        "\n",
        "    '''\n",
        "    Post process the output for one image across one FPN level\n",
        "    Input:\n",
        "       mat_clas: {(1*num_anchors,grid_size[0],grid_size[1])}  (score of the output boxes)\n",
        "       mat_coord: {(4*num_anchors,grid_size[0],grid_size[1])} (encoded coordinates of the output boxess)\n",
        "       anchors_list[lvl] (num_anchors,grid_size[0],grid_size[1],4)\n",
        "    Output:\n",
        "        nms_clas: (Post_NMS_boxes)\n",
        "        nms_prebox: (Post_NMS_boxes,4)\n",
        "    '''\n",
        "    def postprocessImg(self,mat_clas, mat_coord, anchors, IOU_thresh, keep_num_postNMS):\n",
        "        #decode mat_coord from tx, ty, tw, th to normal x,y,w,h \n",
        "        #preparing coordinates to match anchors\n",
        "        temp_coords = []\n",
        "        for a in range(self.num_anchors):\n",
        "          temp_coords.append(mat_coord[a:a+4])\n",
        "        reshaped_coords = torch.stack(temp_coords).permute(0,2,3,1).reshape(-1,4)\n",
        "        anchors = anchors.reshape(-1,4).to(device)   #flattening\n",
        "        xa = anchors[:,0]\n",
        "        ya = anchors[:,1]\n",
        "        wa = anchors[:,2]\n",
        "        ha = anchors[:,3]\n",
        "        tx, ty, tw, th = reshaped_coords[:,0],  reshaped_coords[:,1], reshaped_coords[:,2], reshaped_coords[:,3]\n",
        "        x, y, w, h = tx*wa+xa, ty*ha+ya, torch.exp(tw)*wa, torch.exp(th)*ha\n",
        "        #convert to x1,y1,x2,y2 format\n",
        "        x1, y1, x2, y2 = x- w/2, y-h/2, x+w/2, y+h/2\n",
        "        #remove invalid upper and lower bounds\n",
        "        cross_bound_l = torch.logical_or(x1<=0,y1<=0)\n",
        "        cross_bound_h = torch.logical_or(x2>=1088.0,y2>=800)\n",
        "        mat_clas = mat_clas.flatten()\n",
        "        mat_clas[cross_bound_l]=0\n",
        "        mat_clas[cross_bound_h]=0\n",
        "\n",
        "        #take only the objectness that is >0.5\n",
        "        thresh_obj = mat_clas[mat_clas>0.5]\n",
        "        #take corresponding x1,y1,x2,y2\n",
        "        x1, y1, x2, y2 = x1[mat_clas>0.5], y1[mat_clas>0.5], x2[mat_clas>0.5], y2[mat_clas>0.5]\n",
        "\n",
        "        num_ele = len(thresh_obj)\n",
        "        #take only top keep_num_preNMS boxes in sorted form to pass to nms\n",
        "        id = torch.argsort(thresh_obj, descending=True)[:num_ele]\n",
        "        prebox = torch.vstack((x1[id], y1[id], x2[id], y2[id]))\n",
        "        clas = thresh_obj[id]\n",
        "        # call NMS \n",
        "        post_nms_class, post_nms_bboxes = self.NMS(clas, prebox, IOU_thresh)\n",
        "        post_nms_class = np.array(post_nms_class)\n",
        "        post_nms_bboxes = np.array(post_nms_bboxes)\n",
        "        keep = min(keep_num_postNMS, post_nms_class.shape[0])\n",
        "        nms_clas = post_nms_class[:keep]\n",
        "        nms_prebox = post_nms_bboxes[:keep]\n",
        "\n",
        "        prebox = prebox.T.cpu().detach().numpy()\n",
        "        clas = clas.cpu().detach().numpy()\n",
        "\n",
        "        return nms_clas, nms_prebox, clas, prebox\n",
        "\n",
        "    '''\n",
        "    Post process for the outputs for a batch of images\n",
        "    Input:\n",
        "          out_c: list:len(FPN){(bz,1*num_anchors,grid_size[0],grid_size[1])}\n",
        "          out_r: list:len(FPN){(bz,4*num_anchors,grid_size[0],grid_size[1])}\n",
        "          IOU_thresh: scalar that is the IOU threshold for the NMS\n",
        "          keep_num_postNMS: number of masks we will keep from each image after the NMS\n",
        "    Output:\n",
        "          nms_clas_list: list:len(bz){(Post_NMS_boxes)} (the score of the boxes that the NMS kept)\n",
        "          nms_prebox_list: list:len(bz){(Post_NMS_boxes,4)} (the coordinate of the boxes that the NMS kept)\n",
        "    '''\n",
        "    def postprocess(self, out_c, out_r, IOU_thresh=0.5, keep_num_postNMS=100):\n",
        "        nms_class_list = []\n",
        "        nms_box_list = []\n",
        "        clas_list = []\n",
        "        prebox_list = []\n",
        "        anchors_list = self.anchors\n",
        "        for img_id in range(len(out_c[0])): #iterate over each image in the batch_size\n",
        "          lvl_pre_cls = []\n",
        "          lvl_pre_box = []\n",
        "          lvl_post_cls = []\n",
        "          lvl_post_box = []\n",
        "          for lvl in range(len(out_c)):  #iterate over FPN each level         \n",
        "            nms_class, nms_box, clas, prebox = self.postprocessImg(out_c[lvl][img_id], out_r[lvl][img_id], anchors_list[lvl], IOU_thresh, keep_num_postNMS)\n",
        "            if torch.tensor(nms_class).shape[0]!=0:\n",
        "              lvl_post_cls.append(torch.tensor(nms_class))\n",
        "            if torch.tensor(nms_box).shape[0]!=0:\n",
        "              lvl_post_box.append(torch.tensor(nms_box))\n",
        "            if torch.tensor(clas).shape[0]!=0:\n",
        "              lvl_pre_cls.append(torch.tensor(clas))\n",
        "            if torch.tensor(prebox).shape[0]!=0:\n",
        "              lvl_pre_box.append(torch.tensor(prebox))\n",
        "          nms_class_list.append(torch.cat(lvl_post_cls))\n",
        "          nms_box_list.append(torch.cat(lvl_post_box))\n",
        "          clas_list.append(torch.cat(lvl_pre_cls))\n",
        "          prebox_list.append(torch.cat(lvl_pre_box))\n",
        "        return nms_class_list, nms_box_list, clas_list, prebox_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU1vV3TKJxzn"
      },
      "source": [
        "## Visualization of ground truth for 1 example (only on CPU for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYXVfn1f4HQE"
      },
      "outputs": [],
      "source": [
        "#before implementing the loss function lets visualize the ground truth (works only in cpu)\n",
        "rpn = RPNHead()\n",
        "anchors_list = rpn.get_anchors()\n",
        "image, _, _, bboxes, _ = dataset.__getitem__(1)\n",
        "# transed_img, labels, transed_masks, transed_bboxes, index\n",
        "g_class, g_coord = rpn.create_ground_truth(bboxes = bboxes, grid_sizes = rpn.grid_size,anchors_list = anchors_list)\n",
        "\n",
        "#display original image2\n",
        "for lvl in range(len(g_class)):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot()\n",
        "  ax.imshow(image.detach().cpu().numpy().transpose(1,2,0))\n",
        "  for a in range(3):\n",
        "    #prepare corresponding anchors \n",
        "    flat_anchors = anchors_list[lvl][a].permute(2,0,1).flatten(start_dim=1,end_dim=-1).T\n",
        "    flat_ground_coord = g_coord[lvl][a:a+4].flatten(start_dim=1,end_dim=-1)\n",
        "    flat_ground_class = g_class[lvl][a].reshape(-1)\n",
        "    objects = (flat_ground_class==1).nonzero().flatten()\n",
        "    #print(objects)\n",
        "\n",
        "    #decoding object wise\n",
        "    for elem in objects:\n",
        "          x_a, y_a, w_a, h_a = flat_anchors[elem, 0], flat_anchors[elem, 1], flat_anchors[elem, 2], flat_anchors[elem, 3]\n",
        "          tx, ty, tw, th = flat_ground_coord[0, elem], flat_ground_coord[1, elem], flat_ground_coord[2, elem], flat_ground_coord[3, elem]\n",
        "          w = torch.exp(tw) * w_a\n",
        "          h = torch.exp(th) * h_a\n",
        "          x = (tx * w_a) + x_a\n",
        "          y = (ty * h_a) + y_a\n",
        "          x = x.to('cpu')\n",
        "          y = y.to('cpu')\n",
        "          w = w.to('cpu')\n",
        "          h = h.to('cpu')\n",
        "          rect = patches.Rectangle((x-w/2,y-h/2), w, h,fill=False,color='r')\n",
        "          # rect = rect.to('cpu')\n",
        "          ax.add_patch(rect)\n",
        "          rect = patches.Rectangle((x_a-w_a/2, y_a-h_a/2), w_a, h_a, fill=False,color='b')\n",
        "          ax.add_patch(rect)\n",
        "          \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnAXTJJ6LB8L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXp2UXnbLUds"
      },
      "outputs": [],
      "source": [
        "rpn_network_obj = RPNHead().to(device)\n",
        "optimizer = torch.optim.Adam(rpn_network_obj.parameters(), lr = 0.001)\n",
        "\n",
        "train_loss = []\n",
        "train_loss_c = []\n",
        "train_loss_r = []\n",
        "val_loss = []\n",
        "val_loss_c = []\n",
        "val_loss_r = []\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  run_loss = 0\n",
        "  run_loss_c = 0\n",
        "  run_loss_r = 0\n",
        "  #train the model\n",
        "  rpn_network_obj.train()\n",
        "  for i, (images, labels, masks, bboxes, indexes) in enumerate(train_loader):\n",
        "    #pass it through the network\n",
        "    clas, regr = rpn_network_obj.forward(images.to(device))\n",
        "    #get gt\n",
        "    gt_clas_batch, gt_coord_batch = rpn_network_obj.create_batch_truth(bboxes)\n",
        "    #zero out optimizer\n",
        "    optimizer.zero_grad()\n",
        "    #calc the loss\n",
        "    loss, loss_c , loss_r = rpn_network_obj.compute_loss(clas, regr, gt_clas_batch, gt_coord_batch)\n",
        "    #backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # empty intermediate predcitions \n",
        "    del clas, regr, gt_clas_batch, gt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    run_loss += loss.item()\n",
        "    run_loss_c += loss_c.item()\n",
        "    run_loss_r += loss_r.item()\n",
        "\n",
        "  train_loss.append(run_loss/len(full_loader))\n",
        "  train_loss_c.append(run_loss_c/len(full_loader))\n",
        "  train_loss_r.append(run_loss_r/len(full_loader))\n",
        "\n",
        "  # checkpoint\n",
        "  if(epoch%2==0):\n",
        "    path = \"/content/drive/MyDrive/CIS680/Final/Checkpoint/epoch-\" +str(epoch+1)\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': rpn_network_obj.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': run_loss/len(full_loader),\n",
        "        'clas_loss': run_loss_c/len(full_loader),\n",
        "        'regr_loss': run_loss_r/len(full_loader)\n",
        "        }, path)\n",
        "  \n",
        "  #validating the model\n",
        "  rpn_network_obj.eval()\n",
        "  run_eval_loss = 0\n",
        "  run_eval_loss_c = 0\n",
        "  run_eval_loss_r = 0\n",
        "  for j, (images, labels, masks, bboxes,indexes) in enumerate(test_loader):\n",
        "    vclas, vregr = rpn_network_obj.forward(images.to(device))\n",
        "    vgt_clas_batch, vgt_coord_batch=rpn_network_obj.create_batch_truth(bboxes)\n",
        "    vloss, loss_cls , loss_re = rpn_network_obj.compute_loss(vclas, vregr, vgt_clas_batch, vgt_coord_batch)\n",
        "    run_eval_loss += vloss.item()\n",
        "    run_eval_loss_c += loss_cls.item()\n",
        "    run_eval_loss_r += loss_re.item()\n",
        "    del vclas, vregr, vgt_clas_batch, vgt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  val_loss.append(run_eval_loss/len(full_loader))\n",
        "  print(\"Validation Loss = \", val_loss[-1])\n",
        "  val_loss_c.append(run_eval_loss_c/len(full_loader))\n",
        "  val_loss_r.append(run_eval_loss_r/len(full_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdY9rtEgFf2Z"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_loss_c, label='classifier val loss')\n",
        "plt.plot(val_loss_r, label='regressor val loss')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"category loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0P9QkicDIE6"
      },
      "source": [
        "## Post Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82doF3pwDMzx"
      },
      "outputs": [],
      "source": [
        "path = \"drive/MyDrive/CIS680/Final/Checkpoint/epoch-29\"\n",
        "checkpoint = torch.load(path)\n",
        "test_model = RPNHead().to(device)\n",
        "\n",
        "test_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "#in eval mode\n",
        "test_model.eval()\n",
        "for idx, (images, labels, masks, bboxes, indexes) in enumerate(test_loader):\n",
        "  clas, regr = test_model.forward(images) #predict\n",
        "  post_nms_clas, post_nms_predbox, clas, prebox = test_model.postprocess(clas, regr)  #post process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t8YdonMMnIu"
      },
      "outputs": [],
      "source": [
        "def visualize_post_process(boxes_list, images):\n",
        "  for i in range(len(boxes_list)):    #number of examples to show\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot()\n",
        "    objs = boxes_list[i]\n",
        "    ax.imshow(images[i].detach().cpu().numpy().transpose((1,2,0)))\n",
        "    for box in objs:\n",
        "      x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
        "      rect = rec((x1, y1), (x2 - x1), (y2 - y1), fill=False, color='r')\n",
        "      ax.add_patch(rect)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS7fdlhwMtWV"
      },
      "outputs": [],
      "source": [
        "print(\"Pre NMS\")\n",
        "visualize_post_process(prebox,images)\n",
        "print(\"Post NMS\")\n",
        "visualize_post_process(post_nms_predbox,images)\n",
        "print(clas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVHdRspDD0iE"
      },
      "source": [
        "# Part 5 : Box Head - Old\n",
        "- Lot of this is derived from HW 4B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTNRoWnuECAF"
      },
      "source": [
        "## class BoxHead definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0ghmLM7EIYd"
      },
      "outputs": [],
      "source": [
        "class BoxHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BoxHead, self).__init__()\n",
        "        self.num_classes = 3\n",
        "        self.P = 7\n",
        "        self.intermediate = nn.Sequential(#linear 1\n",
        "                                          nn.Linear(in_features=256*self.P*self.P, out_features=1024),\n",
        "                                          nn.ReLU(),\n",
        "                                          #linear 2\n",
        "                                          nn.Linear(in_features=1024, out_features=1024),\n",
        "                                          nn.ReLU(),\n",
        "                                          )\n",
        "        #maybe softmax is not advised for training in the classifier. verify later\n",
        "        self.classifier = nn.Sequential(\n",
        "                                        nn.Linear(in_features=1024, out_features=self.num_classes + 1), \n",
        "                                        #nn.Softmax()\n",
        "                                        )\n",
        "        self.regressor = nn.Sequential(\n",
        "                                       nn.Linear(in_features=1024, out_features=int(4*self.num_classes))\n",
        "                                      )\n",
        "        \n",
        "    '''\n",
        "    input : proposals:  list len batch_size each tensor of dim (top_k_proposals, 4) [format : (x1, y1, x2, y2)]\n",
        "    output : feature_vectors : (bs*top_k_proposals, 256*P*P) = (total_proposals, 256*P*P)\n",
        "    function is for a batch\n",
        "    '''\n",
        "    def find_feature_vectors(self, batch_proposals, fpn_feat_list):\n",
        "        #regardless of number of objects in each image, there would only be top k proposal boxes fixed. the top_k value is up to us.\n",
        "        feature_vectors = []\n",
        "        #loop over batch\n",
        "        for i in range(len(batch_proposals)):\n",
        "          #loop over each proposal\n",
        "          for p in range(len(batch_proposals[i])):\n",
        "            pb = batch_proposals[i][p]    #these will be 1 proposal\n",
        "            #convert proposal box from x1,y1,x2,y2 to x,y,w,h format\n",
        "            x = (pb[0] + pb[2])/2.0\n",
        "            y = (pb[1] + pb[3])/2.0\n",
        "            w = pb[2] - pb[0]\n",
        "            h = pb[3] - pb[1]\n",
        "            #choose appropriate fpn level to pool features\n",
        "            k = torch.clamp(torch.floor(4 + torch.log2(torch.sqrt(w*h)/224)), 2, 5)\n",
        "            fpn_level = (k - 2.0).int()     #this is a single integer value\n",
        "            #region of proposal box is given in image coords but we need to change them to feature map coords\n",
        "            scale_x = fpn_feat_list[fpn_level].shape[3]/1088\n",
        "            scale_y = fpn_feat_list[fpn_level].shape[2]/800\n",
        "            #scale the proposal box to feature map coords\n",
        "            p_scaled = pb.reshape(1,-1).clone()    #(1,4)\n",
        "            p_scaled[:,0] = p_scaled[:,0] * scale_x\n",
        "            p_scaled[:,1] = p_scaled[:,1] * scale_y\n",
        "            p_scaled[:,2] = p_scaled[:,2] * scale_x\n",
        "            p_scaled[:,3] = p_scaled[:,3] * scale_y\n",
        "            #do ROI align of feature map and proposal box (both in x1,y1,x2,y2 format)\n",
        "            fv  = torchvision.ops.roi_align(fpn_feat_list[fpn_level][i].unsqueeze(0), [p_scaled.to(device)] , output_size=self.P, spatial_scale=1,sampling_ratio=-1)\n",
        "            feature_vectors.append(fv.flatten())  #appended (1 * 256 * P * P) \n",
        "          \n",
        "        feature_vectors = torch.vstack(feature_vectors) #should be (bs * topK, 256*P^2)\n",
        "        return feature_vectors\n",
        "\n",
        "    '''\n",
        "    input : feature_vectors : (total_proposals , 256*P^2) (bs*top_k_proposals = total_proposals)\n",
        "    outputs : class_prob : (total_proposals, num_classes+1)\n",
        "              bbox_reg : (total_proposals, num_classes*4)\n",
        "    '''\n",
        "    def box_head(self, feature_vectors):\n",
        "        X = self.intermediate(feature_vectors)\n",
        "        class_prob = self.classifier(X)\n",
        "        bbox_reg = self.regressor(X)\n",
        "\n",
        "        return class_prob, bbox_reg\n",
        "\n",
        "    '''\n",
        "    input : feature_vectors : (bs, 3, h_img, w_img)\n",
        "    outputs : class_prob : (total_proposals, num_classes+1)\n",
        "              bbox_reg : (total_proposals, num_classes*4)\n",
        "    '''\n",
        "    def forward(self, feature_vectors):\n",
        "        #we do not want to retrain our backbone and rpn so even when eval=False for the rest of the network, it is true for backbone and rpn\n",
        "        #pass feature vectors to box head and get class prob and bbox_regressionss\n",
        "        class_prob, bbox_reg = self.box_head(feature_vectors)\n",
        "        #print(class_prob.shape)\n",
        "        #print(bbox_reg.shape)\n",
        "        return class_prob, bbox_reg\n",
        "    \n",
        "    '''\n",
        "    inputs : bboxes :  (n_objects, 4)\n",
        "             proposals : (top_K proposals, 4) \n",
        "             labels :  (n_objects,)\n",
        "    outputs : gt_class : (top_K proposals, num_classes+1)\n",
        "              gt_coord : (top_K proposals, num_classes*4)\n",
        "    note :\n",
        "    defined for 1 image\n",
        "    bboxes are in the form : x1,y1,x2,y2\n",
        "    proposals are in the form : x1,y1,x2,y2\n",
        "    '''\n",
        "    def create_ground_truth(self, bboxes, proposals, labels):\n",
        "        #print(labels)\n",
        "        #initialize ground truth class and ground truth coords\n",
        "        \n",
        "        gt_class = torch.zeros((proposals.shape[0], self.num_classes+1)).to(device)\n",
        "        gt_coord = torch.zeros((proposals.shape[0], self.num_classes*4)).to(device)\n",
        "        en_gtcoord = torch.zeros((proposals.shape[0], self.num_classes*4)).to(device)\n",
        "        # pdb.set_trace()\n",
        "        #calculte iou\n",
        "        iou = box_iou(proposals,bboxes) #shape is (topK, n_obj)  #  ORDER IS IMPORTANT!\n",
        "        max_iou, idx = torch.max(iou, dim=1)        #find max iou and if that iou is >= 0.5 only then take corresponding index. \n",
        "                                                    #do this for each object in the image\n",
        "        chosen_idx = max_iou>=0.5\n",
        "\n",
        "        #ASSIGNING GROUND TRUTH LABELS\n",
        "        #for the ground truth class, label=channel will hold 1 (one hot encoding)\n",
        "        gt_class[chosen_idx, labels[idx[chosen_idx]]] = 1\n",
        "        #print(gt_class)\n",
        "        #for background we have to do this. big brain time!\n",
        "        bg_idx = torch.all(iou<0.5,dim=1)\n",
        "        gt_class[bg_idx, 0] = 1\n",
        "        #print(gt_class)\n",
        "\n",
        "        #ASSIGNING GROUND TRUTH BBOXES\n",
        "        #for ground truth bboxes we take the following steps\n",
        "        #convert bboxes from x1,y1,x2,y2 to x,y,w,h\n",
        "        converted_bboxes = torch.zeros_like(bboxes)\n",
        "        converted_bboxes[:,0] = (bboxes[:,0] + bboxes[:,2])/2.0\n",
        "        converted_bboxes[:,1] = (bboxes[:,1] + bboxes[:,3])/2.0\n",
        "        converted_bboxes[:,2] = bboxes[:,2] - bboxes[:,0]\n",
        "        converted_bboxes[:,3] = bboxes[:,3] - bboxes[:,1]\n",
        "        #convert all proposals from x1,y1,x2,y2 to x,y,w,h \n",
        "        proposals[:, 0] = (proposals[:, 0] + proposals[:, 2])/2.0\n",
        "        proposals[:, 1] = (proposals[:, 1] + proposals[:, 3])/2.0\n",
        "        proposals[:, 2] = (proposals[:, 2] - proposals[:, 0])\n",
        "        proposals[:, 3] = (proposals[:, 3] - proposals[:, 1])\n",
        "        #encode to tx,ty,tw,th\n",
        "        encoded_bboxes = torch.zeros_like(proposals)\n",
        "        encoded_bboxes[:,0] = (converted_bboxes[idx,0] - proposals[:, 0])/proposals[:, 2]\n",
        "        encoded_bboxes[:,1] = (converted_bboxes[idx,1] - proposals[:, 1])/proposals[:, 3]\n",
        "        encoded_bboxes[:,2] = torch.log(converted_bboxes[idx,2]/proposals[:, 2])\n",
        "        encoded_bboxes[:,3] = torch.log(converted_bboxes[idx,3]/proposals[:, 3])\n",
        "        #assign to gt_coord at appropriate indices\n",
        "        lbl = torch.where(gt_class==1)[1]     #get the labels, including when the label is background (0)\n",
        "        lbl_idx = lbl.nonzero().flatten()     #we want to insert only the non background boxes to the gt_coord matrix. \n",
        "                                              #so we take only those indices which have non zero values\n",
        "        for j in range(len(lbl_idx)):\n",
        "          gt_coord[lbl_idx[j], (lbl[lbl_idx[j]]-1)*4:lbl[lbl_idx[j]]*4] = proposals[j]\n",
        "          en_gtcoord[lbl_idx[j], (lbl[lbl_idx[j]]-1)*4:lbl[lbl_idx[j]]*4] = encoded_bboxes[j]\n",
        "        #print(gt_coord)\n",
        "\n",
        "        return gt_class, gt_coord, en_gtcoord\n",
        "    \n",
        "    '''\n",
        "    inputs : bboxes :  list(bs)(n_objects, 4)\n",
        "             proposals : list(bs)(top_K proposals, 4) \n",
        "             labels :  list (bs)(n_objects,)\n",
        "    outputs : gt_class : (total_proposals, num_classes+1)\n",
        "              gt_coord : (total_proposals, num_classes*4) (bs*top_k_proposals = total_proposals)\n",
        "    '''\n",
        "    def create_batch_truth(self, bboxes, proposals, labels):\n",
        "        gt_class_batch = []\n",
        "        gt_coord_batch = []\n",
        "        for b in range(len(proposals)): #iterate over each image in the batch\n",
        "          gt_class, _, gt_coord = self.create_ground_truth(bboxes[b], proposals[b], labels[b])\n",
        "          gt_class_batch.append(gt_class)\n",
        "          gt_coord_batch.append(gt_coord)\n",
        "        gt_class_batch = torch.vstack(gt_class_batch)\n",
        "        gt_coord_batch = torch.vstack(gt_coord_batch)\n",
        "\n",
        "        return gt_class_batch, gt_coord_batch\n",
        "        \n",
        "    '''\n",
        "    inputs : class_prob : (total_proposals, num_classes+1)\n",
        "             bbox_reg : (total_proposals, num_classes*4)\n",
        "             gt_class : (total_proposals, num_classes+1)\n",
        "             gt_coord : (total_proposals, num_classes*4) (bs*top_k_proposals = total_proposals) these are encoded\n",
        "    outputs : loss, loss_class, loss_reg\n",
        "    notes : loss is for full batch of bs images\n",
        "            labels : vehicle, human, animal means positive; background means negative\n",
        "            make positive and negative stuff only for classifier loss\n",
        "            use only positive classifier stuff for regressor loss\n",
        "            effective batch is basically a subset taken from the batch of proposals (total_proposals me se liya hua batch)\n",
        "    '''\n",
        "    def compute_loss(self, class_prob, bbox_reg, gt_class, gt_coord, l=1, effective_batch=50):\n",
        "        #computing classifier loss on both positive and negative labels\n",
        "        lbl = torch.where(gt_class==1)[1]\n",
        "        l_p = (lbl!=0).nonzero().flatten()\n",
        "        l_n = (lbl==0).nonzero().flatten()\n",
        "        p_size = min(l_p.shape[0], int(effective_batch*3/4))\n",
        "        n_size = min(l_n.shape[0], int(effective_batch/4))\n",
        "        p_id = torch.randperm(l_p.shape[0])[:p_size]\n",
        "        p_id = l_p[p_id]\n",
        "        n_id = torch.randperm(l_n.shape[0])[:n_size]\n",
        "        n_id = l_n[n_id]\n",
        "        p_class_prob = class_prob[p_id,:]   #wherever the column element in zeroth column is NOT 1, take those corresponding rows and use the postive mask (some object is detected)\n",
        "        n_class_prob = class_prob[n_id,:]   #wherever the column element in zeroth column is 1, take those corresponding rows and use the negative mask (this is the background)\n",
        "        p_gt_class = gt_class[p_id,:]\n",
        "        n_gt_class = gt_class[n_id,:]\n",
        "\n",
        "        criterion_class = torch.nn.CrossEntropyLoss()\n",
        "        #doing pos and neg wise CE loss and summing\n",
        "        loss_class_p = criterion_class(p_class_prob, p_gt_class)\n",
        "        loss_class_n = criterion_class(n_class_prob, n_gt_class)\n",
        "        loss_class = loss_class_p + loss_class_n\n",
        "        #print(loss_class)\n",
        "        \n",
        "        #computing regressor loss only on the positive bboxes \n",
        "        p_bbox_reg = bbox_reg[p_id,:]\n",
        "        p_gt_coord = gt_coord[p_id,:]\n",
        "        criterion_reg = torch.nn.SmoothL1Loss()   #default reduction is mean so we do not have to divide by N_reg\n",
        "        #doing class label wise smooth L1 loss and then summing\n",
        "        loss_reg_1 = sum([criterion_reg(p_bbox_reg[:,i], p_gt_coord[:,i]) for i in range(4)])     #vehicle\n",
        "        loss_reg_2 = sum([criterion_reg(p_bbox_reg[:,i], p_gt_coord[:,i]) for i in range(4,8)])   #human\n",
        "        loss_reg_3 = sum([criterion_reg(p_bbox_reg[:,i], p_gt_coord[:,i]) for i in range(8,12)])  #animal\n",
        "        loss_reg = loss_reg_1 + loss_reg_2 + loss_reg_3\n",
        "        #print(loss_reg)\n",
        "\n",
        "        #computing total loss\n",
        "        loss = loss_class + l * loss_reg\n",
        "\n",
        "        return loss, loss_class, loss_reg\n",
        "\n",
        "    '''\n",
        "    for 1 image only (with multiple bboxes and corresponding confidence scores and labels)\n",
        "    inputs : conf len(bs) (keep_num_preNMS)\n",
        "             bboxes len(bs) (keep_num_preNMS,4)\n",
        "             lbl len(bs) (keep_num_preNMS)\n",
        "    outputs : conf len(bs) (keep_num_postNMS)\n",
        "             bboxes len(bs) (keep_num_postNMS,4)\n",
        "             lbl len(bs) (keep_num_postNMS)\n",
        "    '''\n",
        "    def NMS(self, conf, bbox, lbl, thresh):\n",
        "        bbox_clone = bbox.clone().detach()\n",
        "        conf_clone = conf.clone().detach()\n",
        "        lbl_clone = lbl.clone().detach()\n",
        "        nms_box = []\n",
        "        nms_conf = []\n",
        "        nms_lbl = []\n",
        "        bbox_clone = list(bbox_clone)\n",
        "        conf_clone = list(conf_clone)\n",
        "        lbl_clone = list(lbl_clone)\n",
        "\n",
        "        while len(bbox_clone)!=0:\n",
        "          #get the top value of box, conf and labels\n",
        "          curr_box = bbox_clone[0]\n",
        "          curr_conf = conf_clone[0]\n",
        "          curr_lbl = lbl_clone[0]\n",
        "          #remove them from clone lists\n",
        "          bbox_clone.remove(curr_box)\n",
        "          conf_clone.remove(curr_conf)\n",
        "          lbl_clone.remove(curr_lbl)\n",
        "          #append them into corresponding nms lists\n",
        "          nms_box.append(curr_box)\n",
        "          nms_conf.append(curr_conf)\n",
        "          nms_lbl.append(curr_lbl)\n",
        "          for i, other_bboxes in enumerate(bbox_clone):\n",
        "            if(box_iou((curr_box).reshape(1,4), other_bboxes.reshape(1,4))[0]>thresh):\n",
        "              del bbox_clone[i]\n",
        "              del conf_clone[i]\n",
        "              del lbl_clone[i]\n",
        "\n",
        "        #---REVISIT AND FIX----\n",
        "        #for now if the predicted image is not a background then the nms is applied\n",
        "        #if the predicted image is background then the same pre_nms stuff is returned\n",
        "        if nms_conf:\n",
        "          return torch.stack(nms_conf), torch.stack(nms_box), torch.stack(nms_lbl)\n",
        "        else:\n",
        "          return conf, bbox, lbl\n",
        "\n",
        "    '''\n",
        "    inputs : proposals len(bs) (top_K_proposals,4)\n",
        "             classprob (total_proposals,C+1)    total_proposals = topK_proposals * bs\n",
        "             bbox_reg (total_proposals, C*4)\n",
        "    outputs : bboxes len(bs) (keep_num_postNMS,4)\n",
        "              scores len(bs) (keep_num_postNMS)\n",
        "              labels len(bs) (keep_num_postNMS)\n",
        "              #same things for (keep_num_preNMS)\n",
        "    '''\n",
        "    def post_process(self, class_prob, bbox_reg, proposals, conf_thresh = 0.2, keep_num_preNMS = 10, keep_num_postNMS = 5, keep_topK = 200):\n",
        "        postnms_bboxes = []\n",
        "        postnms_scores = []\n",
        "        postnms_labels = []\n",
        "        prenms_bboxes = []\n",
        "        prenms_scores = []\n",
        "        prenms_labels = []\n",
        "        for img_id in range(len(proposals)):\n",
        "          c_prob = class_prob[int(img_id*keep_topK) : int((img_id+1)*keep_topK), :] #(top_K_propsals,C+1)\n",
        "          prop = proposals[img_id]\n",
        "          b_reg = bbox_reg[int(img_id*keep_topK) : int((img_id+1)*keep_topK), :] #(top_K_propsals,C*4)\n",
        "          lbl = torch.argmax(c_prob, axis=1)\n",
        "          conf_max = torch.max(c_prob, axis=1)[0]\n",
        "          #mask is when confidence is more than the threshold and the label is nonzero (so that no background is coming)\n",
        "          mask = torch.logical_and(conf_max>conf_thresh, lbl>0)\n",
        "          #keep only the masked class probabilities, masked labels, masked bbox regressions and masked proposals\n",
        "          m_conf, m_lbl, m_b_reg, m_prop = conf_max[mask], lbl[mask], b_reg[mask], prop[mask]\n",
        "          #keep value will be the min of pre decided preNMS number and the number of confidence scores left\n",
        "          keep = min(keep_num_preNMS, len(m_conf))\n",
        "          #sort the confidence scores in descending order and keep only as many as keep value\n",
        "          kept_idx = torch.argsort(m_conf, dim = 0, descending=True)[:keep]\n",
        "          keep_conf, keep_lbl, keep_b_reg, keep_prop = m_conf[kept_idx], m_lbl[kept_idx], m_b_reg[kept_idx], m_prop[kept_idx]\n",
        "          #transform the bboxes from (keep, C*4) to (keep, 4) for easiness and also for passing into NMS\n",
        "          reshaped_keepbreg = torch.zeros((len(keep_b_reg),4)).to(device)\n",
        "          for b in range(len(keep_b_reg)):\n",
        "            reshaped_keepbreg[b,:] = keep_b_reg[b, int(keep_lbl[b]-1)*4:(int(keep_lbl[b]-1)*4)+4]\n",
        "          #decoding reshaped_keepbreg with corresponding keep_prop (convert keep_prop to x,y,w,h) form and then find x*,y*,w*,h* then convert to x1,y1,x2,y2\n",
        "          decoded = torch.zeros_like(reshaped_keepbreg).to(device)\n",
        "          xp = (keep_prop[:, 0] + keep_prop[:, 2])/2.0\n",
        "          yp = (keep_prop[:, 1] + keep_prop[:, 3])/2.0\n",
        "          wp = (keep_prop[:, 2] - keep_prop[:, 0])\n",
        "          hp = (keep_prop[:, 3] - keep_prop[:, 1])\n",
        "          x = (reshaped_keepbreg[:,0] * wp) + xp\n",
        "          y = (reshaped_keepbreg[:,1] * hp) + yp\n",
        "          w = torch.exp(reshaped_keepbreg[:,2]) * wp\n",
        "          h = torch.exp(reshaped_keepbreg[:,3]) * hp\n",
        "          decoded[:,0] = torch.clip(x-w/2, min=0) #x1\n",
        "          decoded[:,1] = torch.clip(y-h/2, min=0) #y1\n",
        "          decoded[:,2] = torch.clip(x+w/2, max=1088) #x2\n",
        "          decoded[:,3] = torch.clip(y+h/2, max=800) #y2\n",
        "          #collect all prenms bboxes, scores, labels\n",
        "          prenms_bboxes.append(decoded)\n",
        "          prenms_scores.append(keep_conf)\n",
        "          prenms_labels.append(keep_lbl)\n",
        "          #do NMS\n",
        "          after_conf, after_bboxes, after_labels = self.NMS(keep_conf, decoded, keep_lbl, 0.5)\n",
        "          #collect all postnms bboxes, scores, labels\n",
        "          after_keep = min(keep_num_postNMS, len(after_conf))\n",
        "          postnms_bboxes.append(after_bboxes[:after_keep])\n",
        "          postnms_scores.append(after_conf[:after_keep])\n",
        "          postnms_labels.append(after_labels[:after_keep])\n",
        "       \n",
        "        #return pre and post nms bboxes, scores, labels\n",
        "        return prenms_bboxes, prenms_scores, prenms_labels, postnms_bboxes, postnms_scores, postnms_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zJlw9xkloee"
      },
      "source": [
        "## BoxHead Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jSuaQwWJ1L7"
      },
      "outputs": [],
      "source": [
        "def pretrained_models_680(checkpoint_file, eval=True):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)\n",
        "    if(eval):\n",
        "        model.eval()\n",
        "    model.to(device)        \n",
        "    backbone = model.backbone\n",
        "    rpn = model.rpn\n",
        "\n",
        "    if(eval):\n",
        "        backbone.eval()\n",
        "        rpn.eval()\n",
        "\n",
        "    rpn.nms_thresh=0.6\n",
        "    checkpoint = torch.load(checkpoint_file)\n",
        "    backbone.load_state_dict(checkpoint['backbone'])\n",
        "    rpn.load_state_dict(checkpoint['rpn'])\n",
        "\n",
        "    return backbone, rpn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLir9a7Dln5g"
      },
      "outputs": [],
      "source": [
        "network_obj = BoxHead().to(device)\n",
        "optimizer = torch.optim.Adam(network_obj.parameters(), lr = 0.001)\n",
        "\n",
        "train_loss = []\n",
        "train_loss_c = []\n",
        "train_loss_r = []\n",
        "val_loss = []\n",
        "val_loss_c = []\n",
        "val_loss_r = []\n",
        "num_epochs = 20\n",
        "keep_topK = 200\n",
        "path = '/content/drive/MyDrive/CIS680/Final/checkpoint680.pth'\n",
        "print(path)\n",
        "backbone, rpn1 = pretrained_models_680(path)\n",
        "print(backbone)\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  run_loss = 0\n",
        "  run_loss_c = 0\n",
        "  run_loss_r = 0\n",
        "  #train the model\n",
        "  network_obj.train()\n",
        "  for i, (images, labels, masks, bboxes, indexes) in enumerate(train_loader):\n",
        "    #pass through backbone\n",
        "    backout = backbone(images.to(device))\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    #pass thorugh rpn, get proposals and fpn_feat_list\n",
        "    rpnout = rpn1(im_lis, backout)\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list= list(backout.values())\n",
        "    #get gt\n",
        "    gt_class_batch, gt_coord_batch = network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    #get feature vectors\n",
        "    pdb.set_trace()\n",
        "    feature_vectors = network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    #pass through forward\n",
        "    class_prob, bbox_reg = network_obj.forward(feature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN\n",
        "    #zero out optimizer\n",
        "    optimizer.zero_grad()\n",
        "    #calc the loss\n",
        "    #CE loss already does softmax so we dont add it in the classifier network during training\n",
        "    loss, loss_class ,loss_reg = network_obj.compute_loss(class_prob, bbox_reg, gt_class_batch.to(device), gt_coord_batch.to(device),effective_batch=150)\n",
        "    #gather loss values\n",
        "    run_loss += loss.item()\n",
        "    run_loss_c += loss_class.item()\n",
        "    run_loss_r += loss_reg.item()\n",
        "    #backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #empty intermediate predcitions \n",
        "    del class_prob, bbox_reg, gt_class_batch, gt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  train_loss.append(run_loss/len(full_loader))\n",
        "  train_loss_c.append(run_loss_c/len(full_loader))\n",
        "  train_loss_r.append(run_loss_r/len(full_loader))\n",
        "\n",
        "  # checkpoint\n",
        "  if(epoch%2==0):\n",
        "    path = \"/content/drive/MyDrive/CIS680/Final/Box1812/epoch-\" +str(epoch+1)\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': network_obj.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': run_loss/len(full_loader),\n",
        "        'clas_loss': run_loss_c/len(full_loader),\n",
        "        'regr_loss': run_loss_r/len(full_loader)\n",
        "        }, path)\n",
        "  \n",
        "  #validating the model\n",
        "  network_obj.eval()\n",
        "  run_eval_loss = 0\n",
        "  run_eval_loss_c = 0\n",
        "  run_eval_loss_r = 0\n",
        "  for j, (images, labels, masks, bboxes,indexes) in enumerate(test_loader):\n",
        "    #pass through backbone\n",
        "    backout = backbone(images.to(device))\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    #pass thorugh rpn, get proposals and fpn_feat_list\n",
        "    rpnout = rpn1(im_lis, backout)\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list= list(backout.values())\n",
        "    #get gt\n",
        "    vgt_class_batch, vgt_coord_batch = network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    #get feature vectors\n",
        "    vfeature_vectors = network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    #pass through forward\n",
        "    vclass_prob, vbbox_reg = network_obj.forward(vfeature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN\n",
        "    #calc the loss\n",
        "    vloss, vloss_class ,vloss_reg = network_obj.compute_loss(vclass_prob, vbbox_reg, vgt_class_batch.to(device), vgt_coord_batch.to(device),effective_batch=150)\n",
        "    run_eval_loss += vloss.item()\n",
        "    run_eval_loss_c += vloss_class.item()\n",
        "    run_eval_loss_r += vloss_reg.item()\n",
        "    del vclass_prob, vbbox_reg, vgt_class_batch, vgt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  val_loss.append(run_eval_loss/len(full_loader))\n",
        "  val_loss_c.append(run_eval_loss_c/len(full_loader))\n",
        "  val_loss_r.append(run_eval_loss_r/len(full_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8caWp1OhD3gF"
      },
      "outputs": [],
      "source": [
        "train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv7FN0UwLmtH"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "\n",
        "ax[0].plot(train_loss)\n",
        "ax[0].set_title('Total Train Loss')\n",
        "ax[0].set_xlabel('epochs')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].grid()\n",
        "\n",
        "ax[1].plot(val_loss)\n",
        "ax[1].set_title('Total Validation Loss')\n",
        "ax[1].set_xlabel('epochs')\n",
        "ax[1].set_ylabel('loss')\n",
        "ax[1].grid()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/model_trained_boxhead.pth\"\n",
        "checkpoint = torch.load(path)\n",
        "boxhead_network_obj = BoxHead().to(device)\n",
        "# odict_keys(['intermediate_layer.0.weight', 'intermediate_layer.0.bias', 'intermediate_layer.2.weight', 'intermediate_layer.2.bias', 'classifier.0.weight', 'classifier.0.bias', 'regressor.0.weight', 'regressor.0.bias'])\n",
        "for key in list(checkpoint.keys()):\n",
        "    checkpoint[key.replace('intermediate_layer', 'intermediate')] = checkpoint.pop(key)\n",
        "for key in list(checkpoint.keys()):\n",
        "    checkpoint[key.replace('classifier_head', 'classifier')] = checkpoint.pop(key)\n",
        "for key in list(checkpoint.keys()):\n",
        "    checkpoint[key.replace('regressor_head', 'regressor')] = checkpoint.pop(key)\n",
        "print(checkpoint.keys())\n",
        "boxhead_network_obj.load_state_dict(checkpoint, strict = False)"
      ],
      "metadata": {
        "id": "au6shnmJbgwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm-YIptHWRwh"
      },
      "outputs": [],
      "source": [
        "topK = 200\n",
        "network_obj.eval()\n",
        "softmax = nn.Softmax()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (images, labels, masks, bboxes, indexes) in enumerate(test_loader):\n",
        "    backout = backbone(images)\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    rpnout = rpn1(im_lis, backout)\n",
        "    proposals=[proposal[0:topK,:].to(device) for proposal in rpnout[0]]\n",
        "    # proposals=[proposal[0:topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list= list(backout.values())\n",
        "    #get feature vectors\n",
        "    feature_vectors = network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    class_prob, bbox_reg = network_obj(feature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN\n",
        "    cls_soft = softmax(class_prob)    #do softmax now as we did not do it in forward function\n",
        "    #getmax confidence score and corresponding index\n",
        "    print(\"class_prob = \\n\", class_prob)\n",
        "    cls_with_softmax, max_indices = torch.max(cls_soft,dim=1)  \n",
        "    print(\"max_indices = \", max_indices)\n",
        "    for i in range(len(images)):\n",
        "        #display the image\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot()\n",
        "        ax.imshow(images[i].detach().cpu().numpy().transpose(1,2,0))\n",
        "        print(\"___________\")\n",
        "        cls_img, lbl = cls_with_softmax[i*topK:(i+1)*topK], max_indices[i*topK:(i+1)*topK]\n",
        "        #iterate over proposals\n",
        "        print(\"Proposals = \", len(proposals))\n",
        "        for j in range(len(proposals)):\n",
        "          #if label is 0, it is background so ignore\n",
        "          if(lbl[j]==0): \n",
        "            print(\"Background proposal \", j)\n",
        "            continue\n",
        "          #get single proposal and bounding box\n",
        "          edge_ind = (bbox_reg[j][((lbl[j]-1)*4).int() : ((lbl[j]-1)*4+4).int()]).cpu()\n",
        "          prop = (proposals[i][j]).cpu()\n",
        "          #decode the bounding box with corresponding proposal\n",
        "          tx, ty, tw, th = edge_ind[0], edge_ind[1], edge_ind[2], edge_ind[3]\n",
        "          xp, yp, wp, hp = (prop[0]+prop[2])/2, (prop[1]+prop[3])/2, prop[2]-prop[0], prop[3]-prop[1]\n",
        "          x, y, w, h = tx*wp+xp, ty*hp+yp, torch.exp(tw)*wp, torch.exp(th)*hp\n",
        "          x1, y1 = x-(w/2), y-(h/2)\n",
        "          print(x,y,w,h)\n",
        "          print(\"--------------------\")\n",
        "          #plot the top 20 predicted bbox regressions\n",
        "          if (lbl[j]==1): #----vehicle\n",
        "            rect = rec((x1, y1),w,h,fill=False,color='r',linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "          if (lbl[j]==2): #----human\n",
        "            rect = rec((x1, y1),w,h,fill=False,color='g',linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "          if (lbl[j]==3): #----animal\n",
        "            rect = rec((x1, y1),w,h,fill=False,color='b',linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "        plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-dK6-nlXTFv"
      },
      "source": [
        "# Part 6 : Mask Head - Raima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrIDN8INXhZo"
      },
      "source": [
        "## class MaskHead definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CexrvvfXchZ"
      },
      "outputs": [],
      "source": [
        "class MaskHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskHead, self).__init__()\n",
        "        self.num_classes = 3\n",
        "        self.P = 14\n",
        "        self.layers1to4 = nn.Sequential( nn.Conv2d(in_channels=256 , out_channels= 256, kernel_size= 3, padding= 'same'),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Conv2d(in_channels=256 , out_channels= 256, kernel_size= 3, padding= 'same'),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Conv2d(in_channels=256 , out_channels= 256, kernel_size= 3, padding= 'same'),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Conv2d(in_channels=256 , out_channels= 256, kernel_size= 3, padding= 'same'),\n",
        "                                         nn.ReLU(),\n",
        "                                          )\n",
        "        self.deconv = nn.Sequential( nn.ConvTranspose2d(in_channels=256 , out_channels= 256, kernel_size= 3, stride=2, padding= 1, output_padding=1),\n",
        "                                    nn.ReLU())\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_channels=256 , out_channels= self.num_classes, kernel_size= 1),\n",
        "                                  nn.Sigmoid())\n",
        "        \n",
        "    '''\n",
        "    input : proposals:  list len batch_size each tensor of dim (top_k_proposals, 4) [format : (x1, y1, x2, y2)]\n",
        "    output : feature_vectors : (bs*top_k_proposals, 256*P*P) = (total_proposals, 256*P*P)\n",
        "    function is for a batch\n",
        "    '''\n",
        "    def find_feature_vectors(self, batch_proposals, fpn_feat_list):\n",
        "        #regardless of number of objects in each image, there would only be top k proposal boxes fixed. the top_k value is up to us.\n",
        "        feature_vectors = []\n",
        "        #loop over batch\n",
        "        for i in range(len(batch_proposals)):\n",
        "          #loop over each proposal\n",
        "          for p in range(len(batch_proposals[i])):\n",
        "            pb = batch_proposals[i][p]    #these will be 1 proposal\n",
        "            #convert proposal box from x1,y1,x2,y2 to x,y,w,h format\n",
        "            x = (pb[0] + pb[2])/2.0\n",
        "            y = (pb[1] + pb[3])/2.0\n",
        "            w = pb[2] - pb[0]\n",
        "            h = pb[3] - pb[1]\n",
        "            #choose appropriate fpn level to pool features\n",
        "            k = torch.clamp(torch.floor(4 + torch.log2(torch.sqrt(w*h)/224)), 2, 5)\n",
        "            fpn_level = (k - 2.0).int()     #this is a single integer value\n",
        "            #region of proposal box is given in image coords but we need to change them to feature map coords\n",
        "            # pdb.set_trace()\n",
        "            scale_x = fpn_feat_list[fpn_level].shape[3]/1088\n",
        "            scale_y = fpn_feat_list[fpn_level].shape[2]/800\n",
        "            #scale the proposal box to feature map coords\n",
        "            p_scaled = pb.reshape(1,-1).clone()    #(1,4)\n",
        "            p_scaled[:,0] = p_scaled[:,0] * scale_x\n",
        "            p_scaled[:,1] = p_scaled[:,1] * scale_y\n",
        "            p_scaled[:,2] = p_scaled[:,2] * scale_x\n",
        "            p_scaled[:,3] = p_scaled[:,3] * scale_y\n",
        "            #do ROI align of feature map and proposal box (both in x1,y1,x2,y2 format)\n",
        "            fv  = torchvision.ops.roi_align(fpn_feat_list[fpn_level][i].unsqueeze(0), [p_scaled.to(device)] , output_size=self.P, spatial_scale=1,sampling_ratio=-1)\n",
        "            # print(\"------\\nIn Mask Head\\nfv = \", fv.shape)\n",
        "            # print(\"------------\")\n",
        "            feature_vectors.append(fv)  #appended (1 * 256 * P * P) \n",
        "          \n",
        "        feature_vectors = torch.vstack(feature_vectors) #should be (bs * topK, 256*P^2)\n",
        "        return feature_vectors\n",
        "      # This function decodes the output that is given in the encoded format (defined in the handout)\n",
        "      # into box coordinates where it returns the upper left and lower right corner of the proposed box\n",
        "      # Input:\n",
        "      #       flatten_out: (total_number_of_anchors*bz,4)\n",
        "      #       flatten_anchors: (total_number_of_anchors*bz,4)\n",
        "      # Output:\n",
        "      #       box: (total_number_of_anchors*bz,4)\n",
        "    def output_decoding_postprocess(self, flatten_out,flatten_anchors, device=device):\n",
        "          #######################################\n",
        "          # TODO decode the output\n",
        "          \n",
        "          fin_box = torch.zeros_like(flatten_anchors)\n",
        "          fin_box[:,0] = (flatten_anchors[:,0] + flatten_anchors[:,2]) / 2\n",
        "          fin_box[:,1] = (flatten_anchors[:,1] + flatten_anchors[:,3]) / 2\n",
        "          fin_box[:,2] = (flatten_anchors[:,2] - flatten_anchors[:,0])\n",
        "          fin_box[:,3] = (flatten_anchors[:,3] - flatten_anchors[:,1])\n",
        "          flatten_anchors = fin_box\n",
        "\n",
        "          conv_box = torch.zeros_like(flatten_anchors).to(device)\n",
        "                   \n",
        "          conv_box[:,3] = torch.exp(flatten_out[:,3]) * flatten_anchors[:,3]\n",
        "          conv_box[:,2] = torch.exp(flatten_out[:,2]) * flatten_anchors[:,2]\n",
        "          conv_box[:,1] = (flatten_out[:,1] * flatten_anchors[:,2]) + flatten_anchors[:,1]\n",
        "          conv_box[:,0] = (flatten_out[:,0] * flatten_anchors[:,3]) + flatten_anchors[:,0]\n",
        "\n",
        "          # COnvert from xywh to x1 y1 x2 y2\n",
        "          box = torch.zeros_like(conv_box)\n",
        "          box[:,0] = conv_box[:,0] - (conv_box[:,2]/2)\n",
        "          box[:,1] = conv_box[:,1] - (conv_box[:,3]/2)\n",
        "          box[:,2] = conv_box[:,0] + (conv_box[:,2]/2)\n",
        "          box[:,3] = conv_box[:,1] + (conv_box[:,3]/2)\n",
        "          return box\n",
        "\n",
        "    def output_decoding(self, flatten_out, flatten_anchors, device='cpu'):\n",
        "        # This function decodes the output that are given in the [t_x,t_y,t_w,t_h] format\n",
        "        # into box coordinates where it returns the upper left and lower right corner of the bbox\n",
        "        # Input:\n",
        "        #       flatten_out: (total_number_of_anchors*bz,4)\n",
        "        #       flatten_anchors: (total_number_of_anchors*bz,4)\n",
        "        # Output:\n",
        "        #       box: (total_number_of_anchors*bz,4)\n",
        "        conv_box = torch.zeros_like(flatten_anchors)\n",
        "        conv_box[:,3] = torch.exp(flatten_out[:,3]) * flatten_anchors[:,3]\n",
        "        conv_box[:,2] = torch.exp(flatten_out[:,2]) * flatten_anchors[:,2]\n",
        "        conv_box[:,1] = (flatten_out[:,1] * flatten_anchors[:,3]) + flatten_anchors[:,1]\n",
        "        conv_box[:,0] = (flatten_out[:,0] * flatten_anchors[:,2]) + flatten_anchors[:,0]\n",
        "\n",
        "        # box = conv_box_to_corners(conv_box)\n",
        "        box = torch.zeros_like(conv_box)\n",
        "        box[:,0] = conv_box[:,0] - (conv_box[:,2]/2)\n",
        "        box[:,1] = conv_box[:,1] - (conv_box[:,3]/2)\n",
        "        box[:,2] = conv_box[:,0] + (conv_box[:,2]/2)\n",
        "        box[:,3] = conv_box[:,1] + (conv_box[:,3]/2)\n",
        "\n",
        "        return box\n",
        " \n",
        "\n",
        "    def preprocess_ground_truth_creation(self, proposals, class_logits, box_regression, gt_labels,bbox ,masks , IOU_thresh=0.5, keep_num_preNMS=1000, keep_num_postNMS=10):\n",
        "        '''This function does the pre-prossesing of the proposals created by the Box Head (during the training of the Mask Head)\n",
        "        and create the ground truth for the Mask Head\n",
        "        \n",
        "        Input:\n",
        "              class_logits: (total_proposals,(C+1))\n",
        "              box_regression: (total_proposal,4*C)           ([t_x,t_y,t_w,t_h] format)\n",
        "              proposals: list:len(bz)(per_image_proposals,4) (the proposals are produced from RPN [x1,y1,x2,y2] format)\n",
        "              conf_thresh: scalar\n",
        "              keep_num_preNMS: scalar (number of boxes to keep pre NMS)\n",
        "              keep_num_postNMS: scalar (number of boxes to keep post NMS)\n",
        "        Output:\n",
        "              boxes: list:len(bz){(post_NMS_boxes_per_image,4)} ([x1,y1,x2,y2] format)\n",
        "              scores: list:len(bz){(post_NMS_boxes_per_image)}   ( the score for the top class for the regressed box)\n",
        "              labels: list:len(bz){(post_NMS_boxes_per_image)}  (top category of each regressed box)\n",
        "              gt_masks: list:len(bz){(post_NMS_boxes_per_image,C,2*P,2*P)}'''\n",
        "        \n",
        "        \n",
        "        num_proposals = proposals[0].shape[0]\n",
        "        # print(\"scores  : \", class_logits)\n",
        "        boxes = []\n",
        "        scores = []\n",
        "        labels = []\n",
        "        gt_masks = []\n",
        "        for i, each_proposal in enumerate(proposals):\n",
        "            \n",
        "            each_proposal = each_proposal.to(device)\n",
        "            box_regression = box_regression.to(device)\n",
        "            class_logits = class_logits.to(device)\n",
        "            # gt_labels[i] = gt_labels[i] - 1\n",
        "            one_image_boxes = box_regression[i*num_proposals:(i+1)*num_proposals]          # Shape (num_proposals, 12)\n",
        "            one_image_logits = class_logits[i*num_proposals:(i+1)*num_proposals]           # Shape (num_proposals, 4)\n",
        "            one_image_scores, one_image_label = torch.max(one_image_logits, dim=1)\n",
        "            one_image_label = one_image_label.clone().int() - 1\n",
        "            non_bg_label_idx = torch.where(one_image_label >= 0)[0]\n",
        "\n",
        "            if len(non_bg_label_idx) != 0: \n",
        "                class_labels = one_image_label[non_bg_label_idx]\n",
        "                all_class_boxes = one_image_boxes[non_bg_label_idx]\n",
        "\n",
        "                #Get the boxes corresponding to the labels \n",
        "\n",
        "                class_boxes =  torch.stack([all_class_boxes[i, x*4:(x+1)*4] for i, x in enumerate(class_labels)])      # Shape(filtered_labels, 4) ([t_x,t_y,t_w,t_h])\n",
        "                decoded_boxes = self.output_decoding_postprocess(class_boxes, each_proposal[non_bg_label_idx])                          # (x1,y1,x2,y2)\n",
        "                \n",
        "                valid_boxes_idx = torch.where((decoded_boxes[:,0] >= 0) & (decoded_boxes[:,2] < 1088) & (decoded_boxes[:,1] > 0) & (decoded_boxes[:,3] < 800))\n",
        "                valid_boxes = decoded_boxes[valid_boxes_idx]\n",
        "\n",
        "                valid_clases = one_image_label[non_bg_label_idx][valid_boxes_idx]\n",
        "                valid_scores = one_image_scores[non_bg_label_idx][valid_boxes_idx]\n",
        "                sorted_scores_pre_nms, sorted_idx = torch.sort(valid_scores, descending=True)\n",
        "                sorted_clases_pre_nms = valid_clases[sorted_idx]\n",
        "\n",
        "                #Rearrange the boxes from x2 y2 x1 y1\n",
        "\n",
        "                sorted_boxes_pre_nms = sorted_boxes_pre_nms[:, [2,3,0,1]]\n",
        "\n",
        "                sorted_boxes_pre_nms = valid_boxes[sorted_idx]\n",
        "\n",
        "                iou_check = box_iou(sorted_boxes_pre_nms, bbox[i])\n",
        "                iou_idx = (iou_check > 0.3).nonzero()\n",
        "                above_thres_idx = iou_idx[:,0]\n",
        "                above_thres_gt = iou_idx[:,1]\n",
        "\n",
        "\n",
        "                masks_gt_all = masks[i][above_thres_gt]\n",
        "                                \n",
        "                sorted_boxes_pre_nms = sorted_boxes_pre_nms[above_thres_idx]\n",
        "                sorted_clases_pre_nms = sorted_clases_pre_nms[above_thres_idx]\n",
        "                sorted_scores_pre_nms = sorted_scores_pre_nms[above_thres_idx]\n",
        "\n",
        "                if len(sorted_clases_pre_nms) > keep_num_preNMS:\n",
        "                    clases_pre_nms = sorted_clases_pre_nms[:keep_num_preNMS]\n",
        "                    boxes_pre_nms = sorted_boxes_pre_nms[:keep_num_preNMS]\n",
        "                    scores_pre_nms = sorted_scores_pre_nms[:keep_num_preNMS]\n",
        "                    masks_pre_nms = masks_gt_all[:keep_num_preNMS]\n",
        "                else:\n",
        "                    clases_pre_nms = sorted_clases_pre_nms\n",
        "                    boxes_pre_nms = sorted_boxes_pre_nms\n",
        "                    scores_pre_nms = sorted_scores_pre_nms\n",
        "                    masks_pre_nms = masks_gt_all\n",
        "                pdb.set_trace()\n",
        "                clases_post_nms, scores_post_nms, boxes_post_nms, masks_post_nms = self.nms_preprocess_gt(clases_pre_nms, boxes_pre_nms, scores_pre_nms, masks_pre_nms, IOU_thres=IOU_thresh, keep_num_postNMS=keep_num_postNMS)\n",
        "              \n",
        "                gt_mask_one = torch.zeros(clases_post_nms.shape[0],self.image_size[0], self.image_size[1]).to(device)\n",
        "\n",
        "                for j in range(clases_post_nms.shape[0]):\n",
        "                    b0 = boxes_post_nms[j,0].int()\n",
        "                    b1 = boxes_post_nms[j,1].int()\n",
        "                    b2 = boxes_post_nms[j,2].int()\n",
        "                    b3 = boxes_post_nms[j,3].int()\n",
        "\n",
        "                    gt_mask_one[j , b1:b3 , b0:b2] = 1\n",
        "                gt_mask_one = gt_mask_one * masks_post_nms\n",
        "                gt_mask_one = F.interpolate(gt_mask_one.unsqueeze(0), size=(2*self.P,2*self.P),mode='nearest').squeeze(0)\n",
        "            \n",
        "            gt_masks.append(gt_mask_one)\n",
        "            boxes.append(boxes_post_nms)\n",
        "            scores.append(scores_post_nms)\n",
        "            labels.append(clases_post_nms)\n",
        "\n",
        "        return boxes, scores, labels, gt_masks\n",
        "\n",
        "    def nms_preprocess_gt(self,clases,boxes,scores, masks, IOU_thres=0.5, keep_num_postNMS=100):\n",
        "        # Input:\n",
        "        #       clases: (num_preNMS, )\n",
        "        #       boxes:  (num_preNMS, 4)\n",
        "        #       scores: (num_preNMS,)\n",
        "        # Output:\n",
        "        #       boxes:  (post_NMS_boxes_per_image,4) ([x1,y1,x2,y2] format)\n",
        "        #       scores: (post_NMS_boxes_per_image)   ( the score for the top class for the regressed box)\n",
        "        #       labels: (post_NMS_boxes_per_image)  (top category of each regressed box)\n",
        "        \n",
        "        # TODO - NMS for the given classes, boxes and labels \n",
        "\n",
        "        boxes = boxes.to(device)\n",
        "        clases = clases.to(device)\n",
        "        scores = scores.to(device)\n",
        "        scores_all = [[],[],[]]\n",
        "        boxes_all = [[],[],[]]\n",
        "        clas_all = [[],[],[]]\n",
        "        masks_all = [[],[],[]]\n",
        "\n",
        "        for i in range(3):\n",
        "            each_label_idx = torch.where(clases == i)[0]\n",
        "            if len(each_label_idx) == 0: #Ensures empty list issues are taken care of\n",
        "              continue\n",
        "            each_clas_boxes = boxes[each_label_idx]\n",
        "            each_clas_score = scores[each_label_idx]\n",
        "\n",
        "            start_x_torched = each_clas_boxes[:, 0]\n",
        "            start_y_torched = each_clas_boxes[:, 1]\n",
        "            end_x_torched   = each_clas_boxes[:, 2]\n",
        "            end_y_torched   = each_clas_boxes[:, 3]\n",
        "\n",
        "            areas_torched = (end_x_torched - start_x_torched + 1) * (end_y_torched - start_y_torched + 1)\n",
        "\n",
        "            order_torched = torch.argsort(each_clas_score)\n",
        "\n",
        "            while len(order_torched) > 0:\n",
        "                # The index of largest confidence score\n",
        "                index = order_torched[-1]\n",
        "                \n",
        "                # Pick the bounding box with largest confidence score\n",
        "                boxes_all[i].append(boxes[index].detach())\n",
        "                scores_all[i].append(each_clas_score[index].detach())\n",
        "                masks_all[i].append(masks[index].detach())\n",
        "\n",
        "                if len(boxes_all[i]) == keep_num_postNMS:\n",
        "                    break\n",
        "\n",
        "                # Compute ordinates of intersection-over-union(IOU)\n",
        "                x1 = torch.maximum(start_x_torched[index], start_x_torched[order_torched[:-1]]).to(device)\n",
        "                x2 = torch.minimum(end_x_torched[index], end_x_torched[order_torched[:-1]]).to(device)\n",
        "                y1 = torch.maximum(start_y_torched[index], start_y_torched[order_torched[:-1]]).to(device)\n",
        "                y2 = torch.minimum(end_y_torched[index], end_y_torched[order_torched[:-1]]).to(device)\n",
        "\n",
        "                # Compute areas of intersection-over-union\n",
        "                w = torch.maximum(torch.tensor([0]).to(device), x2 - x1 + 1)\n",
        "                h = torch.maximum(torch.tensor([0]).to(device), y2 - y1 + 1)\n",
        "                intersection = w * h\n",
        "\n",
        "                # Compute the ratio between intersection and union\n",
        "                ratio = intersection / (areas_torched[index] + areas_torched[order_torched[:-1]] - intersection)\n",
        "                left = torch.where(ratio < IOU_thres)[0]\n",
        "                order_torched = order_torched[left]\n",
        "            clas_all[i] = [i]*len(scores_all[i])\n",
        "       \n",
        "        pdb.set_trace()\n",
        "       \n",
        "        fin_masks = torch.cat([torch.stack(one_mask) for one_mask in masks_all if len(one_mask)!=0]).reshape(-1,800,1088) ## ISSUES WITH EMPTY LIST (01-12)\n",
        "        fin_scores = torch.cat([torch.tensor(one_score).reshape(-1,1) for one_score in scores_all if len(one_score)!=0],dim=0).reshape(-1,1)\n",
        "        fin_boxes = torch.cat([torch.stack(one_box) for one_box in boxes_all if len(one_box)!=0]).reshape(-1,4)\n",
        "        fin_clas = torch.cat([torch.tensor(one_clas) for one_clas in clas_all if len(one_clas)!=0]).reshape(-1,1)\n",
        "        return fin_clas, fin_scores, fin_boxes, fin_masks\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, features):\n",
        "        X = self.layers1to4(features)\n",
        "        X = self.deconv(X)\n",
        "        X = self.conv(X)\n",
        "        return X\n",
        "\n",
        "    '''\n",
        "    Input:\n",
        "          clas: (top_k_boxes) (scores of the top k boxes)\n",
        "          prebox: (top_k_boxes,4) (coordinate of the top k boxes)\n",
        "    Output:\n",
        "          nms_clas: (Post_NMS_boxes)\n",
        "          nms_prebox: (Post_NMS_boxes,4)\n",
        "    '''\n",
        "    def NMS(self,clas,prebox, thresh=0.5):\n",
        "        method = 'gauss'\n",
        "        gauss_sigma=0.5\n",
        "        n = len(clas)\n",
        "        sorted_boxs = prebox.reshape(n, -1)\n",
        "        intersection = torch.mm(sorted_boxs, sorted_boxs.T)\n",
        "        areas = sorted_boxs.sum(dim=1).expand(n, n)\n",
        "        union = areas + areas.T - intersection\n",
        "        ious = (intersection / union).triu(diagonal=1)  \n",
        "        ious_cmax = ious.max(0)[0].expand(n, n).T        \n",
        "        if method == 'gauss':\n",
        "            decay = torch.exp(-(ious ** 2 - ious_cmax ** 2) / gauss_sigma)\n",
        "        else:\n",
        "            decay = (1 - ious) / (1 - ious_cmax)       \n",
        "        # move decay to device\n",
        "        decay = decay.min(dim=0)[0].to('cuda')\n",
        "\n",
        "        return clas * decay\n",
        "\n",
        "    '''\n",
        "    general function that takes the input list of tensors and concatenates them along the first tensor dimension\n",
        "    Input:\n",
        "         input_list: list:len(bz){(dim1,?)}\n",
        "    Output:\n",
        "         output_tensor: (sum_of_dim1,?)\n",
        "    '''\n",
        "    def flatten_inputs(self,input_list):\n",
        "        output_tensor = torch.cat(input_list, dim=0)\n",
        "        return output_tensor\n",
        "\n",
        "    '''\n",
        "    This function does the post processing for the result of the Mask Head for a batch of images. It project the predicted mask\n",
        "    back to the original image size\n",
        "    Use the regressed boxes to distinguish between the images\n",
        "    Input:\n",
        "          masks_outputs: (total_boxes,C,2*P,2*P)\n",
        "          boxes: list:len(bz){(post_NMS_boxes_per_image,4)} ([x1,y1,x2,y2] format) ; bz = 1\n",
        "          labels: list:len(bz){(post_NMS_boxes_per_image)}  (top category of each regressed box) ; bz = 1\n",
        "          image_size: tuple:len(2)\n",
        "    Output:\n",
        "          projected_masks: list:len(bz){(post_NMS_boxes_per_image,image_size[0],image_size[1]\n",
        "    '''\n",
        "    def postprocess_mask(self, masks_outputs, boxes, labels, image_size=(800,1088)):\n",
        "        # choose masks that correspond to the classes predicted by the Box Head\n",
        "        boxes_stacked = self.flatten_inputs(boxes)\n",
        "        labels_stacked = self.flatten_inputs(labels)\n",
        "\n",
        "        mask_target = []\n",
        "        for i in range(len(labels_stacked)):\n",
        "            one_mask_output = masks_outputs[i]\n",
        "            temp = one_mask_output[int(labels_stacked[i].item()) - 1, :, :]\n",
        "            temp = torch.nn.functional.interpolate(temp.unsqueeze(dim=0).unsqueeze(dim=0), size=(800,1088), mode='bilinear')\n",
        "            mask_target.append(temp.squeeze().squeeze())\n",
        "            \n",
        "        projected_masks = torch.stack(mask_target)\n",
        "\n",
        "        projected_masks[projected_masks > 0.5] = 1\n",
        "        projected_masks[projected_masks < 0.5] = 0\n",
        "\n",
        "        return projected_masks # This is compared with the gt mask \n",
        "\n",
        "    '''\n",
        "    Compute the total loss of the Mask Head\n",
        "    Input:\n",
        "         mask_output: (total_boxes,C,2*P,2*P)\n",
        "         labels: (total_boxes)\n",
        "         gt_masks: (total_boxes,2*P,2*P) - needs to be created \n",
        "    Output:\n",
        "         mask_loss\n",
        "    '''\n",
        "    def compute_loss(self,mask_output,labels,gt_masks):\n",
        "        mask_target = []\n",
        "        # print(\"\\n-----\\nComputeLoss\")\n",
        "        # print(\"Mask first element = \\n\", mask_output.shape)\n",
        "        # print(\"labels = \", len(labels))\n",
        "        # print(\"gt_masks = \", len(gt_masks))\n",
        "        # print(\"gt_masks[0] = \", gt_masks[0].shape)\n",
        "        gt_masks_cat = []\n",
        "        # print(\"gt_cat_masks = \", gt_masks_cat.shape)\n",
        "        # pdb.set_trace()\n",
        "        mask_target = []\n",
        "        for i in range(len(labels)):\n",
        "            one_mask_output = mask_output[i]\n",
        "            mask_target.append(one_mask_output[int(labels[i].item()) - 1, :, :])\n",
        "        mask_target = torch.stack(mask_target)\n",
        "        \n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        mask_loss = criterion(mask_target, gt_masks)\n",
        "        # for i in range(len(labels)):\n",
        "        #     one_mask_output = mask_output[i]\n",
        "        #     print(\"Mask_output = \", one_mask_output.shape)\n",
        "        #     print(\"labels[i].item() = \", labels[i].cpu().detach())\n",
        "        #     for j in range(labels[i].shape[0]):\n",
        "        #       mask_target.append(one_mask_output[labels[i][j] - 1, :, :])\n",
        "        #       temp = gt_masks[i][j-1].unsqueeze(0).unsqueeze(0)\n",
        "        #       pdb.set_trace()\n",
        "        #       gt_mask_cat = F.interpolate(temp, (28, 28), mode='bilinear', align_corners=True)\n",
        "        #       # print(\"gt_mask_cat = \", gt_mask_cat.shape)\n",
        "        #       gt_masks_cat.append(gt_mask_cat)\n",
        "\n",
        "        \n",
        "        # mask_target = torch.stack(mask_target)\n",
        "        # # print(\"Mask_target = \", mask_target.shape)\n",
        "        # gt_masks_cat = torch.cat(gt_masks_cat).squeeze()\n",
        "        # # print(\"GT_Maaks_cat = \",gt_masks_cat.shape)\n",
        "        # # gt_masks_cat = F.interpolate(gt_masks_cat, (28, 28), mode='bilinear', align_corners=True)\n",
        "        # criterion = nn.BCELoss()\n",
        "        # mask_loss = criterion(mask_target, gt_masks_cat)\n",
        "        # # print(\"Mask - Loss = \", mask_loss.mean())\n",
        "        return mask_loss.mean()\n",
        "\n",
        "        '''\n",
        "    Compute the total loss of the Mask Head - Focal Loss\n",
        "    Input:\n",
        "         mask_output: (total_boxes,C,2*P,2*P)\n",
        "         labels: (total_boxes)\n",
        "         gt_masks: (total_boxes,2*P,2*P) - needs to be created \n",
        "    Output:\n",
        "         mask_loss\n",
        "    '''\n",
        "    def compute_loss_focal(self,mask_output,labels,gt_masks):\n",
        "        mask_target = []\n",
        "        # print(\"\\n-----\\nComputeLoss\")\n",
        "        # print(\"Mask first element = \\n\", mask_output.shape)\n",
        "        # print(\"labels = \", len(labels))\n",
        "        # print(\"gt_masks = \", len(gt_masks))\n",
        "        # print(\"gt_masks[0] = \", gt_masks[0].shape)\n",
        "        gt_masks_cat = []\n",
        "        # print(\"gt_cat_masks = \", gt_masks_cat.shape)\n",
        "        # pdb.set_trace()\n",
        "        mask_target = []\n",
        "        for i in range(len(labels)):\n",
        "            one_mask_output = mask_output[i]\n",
        "            mask_target.append(one_mask_output[int(labels[i].item()) - 1, :, :])\n",
        "        mask_target = torch.stack(mask_target)\n",
        "        \n",
        "        criterion = focal_loss(gamma = 2.0)\n",
        "\n",
        "        mask_loss = criterion(mask_target, gt_masks)\n",
        "       \n",
        "        return mask_loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUHAD6F3wzgd"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhAawWl20cfa"
      },
      "outputs": [],
      "source": [
        "del boxhead_network_obj\n",
        "del path\n",
        "path = \"/content/model_trained_boxhead.pth\"\n",
        "checkpoint = torch.load(path)\n",
        "boxhead_network_obj = BoxHead().to(device)\n",
        "\n",
        "for key in list(checkpoint.keys()):\n",
        "    checkpoint[key.replace('intermediate_layer', 'intermediate')] = checkpoint.pop(key)\n",
        "for key in list(checkpoint.keys()):\n",
        "    checkpoint[key.replace('classifier_head', 'classifier')] = checkpoint.pop(key)\n",
        "for key in list(checkpoint.keys()):\n",
        "    checkpoint[key.replace('regressor_head', 'regressor')] = checkpoint.pop(key)\n",
        "print(checkpoint.keys())\n",
        "boxhead_network_obj.load_state_dict(checkpoint, strict = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBuMW-DXxVvA"
      },
      "outputs": [],
      "source": [
        "maskhead_network_obj = MaskHead().to(device)\n",
        "optimizer = torch.optim.Adam(maskhead_network_obj.parameters(), lr = 0.001)\n",
        "# boxhead_network_obj\n",
        "train_loss = []\n",
        "train_loss_c = []\n",
        "train_loss_r = []\n",
        "val_loss = []\n",
        "val_loss_c = []\n",
        "val_loss_r = []\n",
        "num_epochs = 20\n",
        "keep_topK = 200\n",
        "# del proposals\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  run_loss = 0\n",
        "  run_loss_c = 0\n",
        "  run_loss_r = 0\n",
        "  #train the model\n",
        "  maskhead_network_obj.train()\n",
        "  for i, (images, labels, masks, bboxes, indexes) in enumerate(train_loader):\n",
        "    proposals = []\n",
        "    del proposals, rpnout1\n",
        "    # Take the features from the backbone\n",
        "    backout = backbone(images)\n",
        "\n",
        "    # The RPN implementation takes as first argument the following image list\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    # Then we pass the image list and the backbone output through the rpn\n",
        "    rpnout1 = rpn1(im_lis, backout)\n",
        "    # pdb.set_trace()\n",
        "    print(\"RPNOUT = \\n\", rpnout)\n",
        "    #The final output is\n",
        "    # A list of proposal tensors: list:len(bz){(keep_topK,4)}\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout1[0]]\n",
        "    print(\"Proposals = \\n\", proposals)\n",
        "    # pdb.set_trace()\n",
        "    # A list of features produces by the backbone's FPN levels: list:len(FPN){(bz,256,H_feat,W_feat)}\n",
        "    fpn_feat_list= list(backout.values())\n",
        "\n",
        "    feature_vectors              = boxhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)      \n",
        "    \n",
        "    class_logits, box_pred       = boxhead_network_obj.forward(feature_vectors)\n",
        "    # pdb.set_trace()\n",
        "    new_labels, regressor_target  = boxhead_network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    # pdb.set_trace()\n",
        "    print(\"Proposals after a while = \\n\", proposals)\n",
        "\n",
        "    mask_boxes, mask_scores, mask_final_labels, mask_gt_masks = maskhead_network_obj.preprocess_ground_truth_creation(proposals, class_logits, box_pred, labels, bboxes ,masks ,     IOU_thresh=0.5, keep_num_preNMS=200, keep_num_postNMS=10)\n",
        "\n",
        "    mask_feature_vectors = maskhead_network_obj.find_feature_vectors(mask_boxes, fpn_feat_list)\n",
        "    mask_output = maskhead_network_obj.forward(mask_feature_vectors)\n",
        "    # pdb.set_trace()\n",
        "    # calculate loss\n",
        "    mask_labels = maskhead_network_obj.flatten_inputs(mask_final_labels)\n",
        "    mask_gt_mks = maskhead_network_obj.flatten_inputs(mask_gt_masks)\n",
        "    # mask_output = maskhead_network_obj.flatten_inputs(mask_output)\n",
        "    pdb.set_trace()\n",
        "    loss = maskhead_network_obj.compute_loss(mask_output.to(device), mask_labels.to(device), mask_gt_mks.to(device))\n",
        "    '''#pass through backbone\n",
        "    backout = backbone(images.to(device))\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    #pass thorugh rpn, get proposals and fpn_feat_list\n",
        "    rpnout = rpn1(im_lis, backout)\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list = list(backout.values())\n",
        "    #get gt\n",
        "    #gt_class_batch, gt_coord_batch = boxhead_network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    #get feature vectors\n",
        "    feature_vectors = maskhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    # print(\"feature_vectors = \", feature_vectors.detach().shape)\n",
        "    #pass through forward\n",
        "    maskout  = maskhead_network_obj.forward(feature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN'''\n",
        "    \n",
        "    # print(\"Maskout = \", maskout.shape)\n",
        "    #zero out optimizer\n",
        "    optimizer.zero_grad()\n",
        "    #calc the loss\n",
        "    #CE loss already does softmax so we dont add it in the classifier network during training\n",
        "    # mask_boxes, mask_scores, mask_labels, mask_gt_masks = maskhead_network_obj.preprocess_ground_truth_creation(label_preds, regr_preds, proposals, labels, bboxes, masks, IOU_thresh=0.5, keep_num_preNMS=700, keep_num_postNMS=100)\n",
        "    # if len(mask_boxes[0]) == 0:\n",
        "    #   continue\n",
        "    # loss= maskhead_network_obj.compute_loss(maskout, labels, masks)\n",
        "    #gather loss values\n",
        "    run_loss += loss.item()\n",
        "    # run_loss_c += loss_class.item()\n",
        "    # run_loss_r += loss_reg.item()\n",
        "    #backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #empty intermediate predcitions \n",
        "    # del gt_class_batch, gt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  train_loss.append(run_loss/len(full_loader))\n",
        "  # train_loss_c.append(run_loss_c/len(full_loader))\n",
        "  # train_loss_r.append(run_loss_r/len(full_loader))\n",
        "\n",
        "  # checkpoint\n",
        "  if(epoch%2==0):\n",
        "    path = \"/content/drive/MyDrive/CIS680/Final/Mask1812/epoch-\" +str(epoch+1)\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': maskhead_network_obj.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': run_loss/len(full_loader),\n",
        "        # 'clas_loss': run_loss_c/len(full_loader),\n",
        "        # 'regr_loss': run_loss_r/len(full_loader)\n",
        "        }, path)\n",
        "  \n",
        "  #validating the model\n",
        "  maskhead_network_obj.eval()\n",
        "  run_eval_loss = 0\n",
        "  run_eval_loss_c = 0\n",
        "  run_eval_loss_r = 0\n",
        "  for j, (images, labels, masks, bboxes,indexes) in enumerate(test_loader):\n",
        "    #pass through backbone\n",
        "    backout = backbone(images.to(device))\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    #pass thorugh rpn, get proposals and fpn_feat_list\n",
        "    rpnout = rpn1(im_lis, backout)\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list= list(backout.values())\n",
        "    #get gt\n",
        "    #vgt_class_batch, vgt_coord_batch = boxhead_network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    #get feature vectors\n",
        "    # vfeature_vectors = maskhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    vfeature_vectors = maskhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    # print(\"feature_vectors = \", vfeature_vectors.detach().shape)\n",
        "    #pass through forward\n",
        "    maskout = maskhead_network_obj.forward(vfeature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN\n",
        "    #calc the loss\n",
        "    vloss= maskhead_network_obj.compute_loss(maskout, labels, masks)\n",
        "\n",
        "    # vloss= maskhead_network_obj.compute_loss(vclass_prob, vbbox_reg, vgt_class_batch.to(device), vgt_coord_batch.to(device),effective_batch=150)\n",
        "    run_eval_loss += vloss.item()\n",
        "    # run_eval_loss_c += vloss_class.item()\n",
        "    # run_eval_loss_r += vloss_reg.item()\n",
        "    # del vclass_prob, vbbox_reg, vgt_class_batch, vgt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  val_loss.append(run_eval_loss/len(full_loader))\n",
        "  # val_loss_c.append(run_eval_loss_c/len(full_loader))\n",
        "  # val_loss_r.append(run_eval_loss_r/len(full_loader))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Focal Loss\n",
        "\n",
        "maskhead_network_obj = MaskHead().to(device)\n",
        "optimizer = torch.optim.Adam(maskhead_network_obj.parameters(), lr = 0.001)\n",
        "# boxhead_network_obj\n",
        "train_loss = []\n",
        "train_loss_c = []\n",
        "train_loss_r = []\n",
        "val_loss = []\n",
        "val_loss_c = []\n",
        "val_loss_r = []\n",
        "num_epochs = 20\n",
        "keep_topK = 200\n",
        "# del proposals\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  run_loss = 0\n",
        "  run_loss_c = 0\n",
        "  run_loss_r = 0\n",
        "  #train the model\n",
        "  maskhead_network_obj.train()\n",
        "  for i, (images, labels, masks, bboxes, indexes) in enumerate(train_loader):\n",
        "    proposals = []\n",
        "    del proposals, rpnout1\n",
        "    # Take the features from the backbone\n",
        "    backout = backbone(images)\n",
        "\n",
        "    # The RPN implementation takes as first argument the following image list\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    # Then we pass the image list and the backbone output through the rpn\n",
        "    rpnout1 = rpn1(im_lis, backout)\n",
        "    # pdb.set_trace()\n",
        "    print(\"RPNOUT = \\n\", rpnout)\n",
        "    #The final output is\n",
        "    # A list of proposal tensors: list:len(bz){(keep_topK,4)}\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout1[0]]\n",
        "    print(\"Proposals = \\n\", proposals)\n",
        "    # pdb.set_trace()\n",
        "    # A list of features produces by the backbone's FPN levels: list:len(FPN){(bz,256,H_feat,W_feat)}\n",
        "    fpn_feat_list= list(backout.values())\n",
        "\n",
        "    feature_vectors              = boxhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)      \n",
        "    \n",
        "    class_logits, box_pred       = boxhead_network_obj.forward(feature_vectors)\n",
        "    # pdb.set_trace()\n",
        "    new_labels, regressor_target  = boxhead_network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    # pdb.set_trace()\n",
        "    print(\"Proposals after a while = \\n\", proposals)\n",
        "\n",
        "    mask_boxes, mask_scores, mask_final_labels, mask_gt_masks = maskhead_network_obj.preprocess_ground_truth_creation(proposals, class_logits, box_pred, labels, bboxes ,masks ,     IOU_thresh=0.5, keep_num_preNMS=200, keep_num_postNMS=10)\n",
        "\n",
        "    mask_feature_vectors = maskhead_network_obj.find_feature_vectors(mask_boxes, fpn_feat_list)\n",
        "    mask_output = maskhead_network_obj.forward(mask_feature_vectors)\n",
        "    # pdb.set_trace()\n",
        "    # calculate loss\n",
        "    mask_labels = maskhead_network_obj.flatten_inputs(mask_final_labels)\n",
        "    mask_gt_mks = maskhead_network_obj.flatten_inputs(mask_gt_masks)\n",
        "    # mask_output = maskhead_network_obj.flatten_inputs(mask_output)\n",
        "    pdb.set_trace()\n",
        "    loss = maskhead_network_obj.compute_loss(mask_output.to(device), mask_labels.to(device), mask_gt_mks.to(device))\n",
        "    '''#pass through backbone\n",
        "    backout = backbone(images.to(device))\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    #pass thorugh rpn, get proposals and fpn_feat_list\n",
        "    rpnout = rpn1(im_lis, backout)\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list = list(backout.values())\n",
        "    #get gt\n",
        "    #gt_class_batch, gt_coord_batch = boxhead_network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    #get feature vectors\n",
        "    feature_vectors = maskhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    # print(\"feature_vectors = \", feature_vectors.detach().shape)\n",
        "    #pass through forward\n",
        "    maskout  = maskhead_network_obj.forward(feature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN'''\n",
        "    \n",
        "    # print(\"Maskout = \", maskout.shape)\n",
        "    #zero out optimizer\n",
        "    optimizer.zero_grad()\n",
        "    #calc the loss\n",
        "    #CE loss already does softmax so we dont add it in the classifier network during training\n",
        "    # mask_boxes, mask_scores, mask_labels, mask_gt_masks = maskhead_network_obj.preprocess_ground_truth_creation(label_preds, regr_preds, proposals, labels, bboxes, masks, IOU_thresh=0.5, keep_num_preNMS=700, keep_num_postNMS=100)\n",
        "    # if len(mask_boxes[0]) == 0:\n",
        "    #   continue\n",
        "    # loss= maskhead_network_obj.compute_loss(maskout, labels, masks)\n",
        "    #gather loss values\n",
        "    run_loss += loss.item()\n",
        "    # run_loss_c += loss_class.item()\n",
        "    # run_loss_r += loss_reg.item()\n",
        "    #backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #empty intermediate predcitions \n",
        "    # del gt_class_batch, gt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  train_loss.append(run_loss/len(full_loader))\n",
        "  # train_loss_c.append(run_loss_c/len(full_loader))\n",
        "  # train_loss_r.append(run_loss_r/len(full_loader))\n",
        "\n",
        "  # checkpoint\n",
        "  if(epoch%2==0):\n",
        "    path = \"/content/drive/MyDrive/CIS680/Final/Mask1812/epoch-\" +str(epoch+1)\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': maskhead_network_obj.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': run_loss/len(full_loader),\n",
        "        # 'clas_loss': run_loss_c/len(full_loader),\n",
        "        # 'regr_loss': run_loss_r/len(full_loader)\n",
        "        }, path)\n",
        "  \n",
        "  #validating the model\n",
        "  maskhead_network_obj.eval()\n",
        "  run_eval_loss = 0\n",
        "  run_eval_loss_c = 0\n",
        "  run_eval_loss_r = 0\n",
        "  for j, (images, labels, masks, bboxes,indexes) in enumerate(test_loader):\n",
        "    #pass through backbone\n",
        "    backout = backbone(images.to(device))\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    #pass thorugh rpn, get proposals and fpn_feat_list\n",
        "    rpnout = rpn1(im_lis, backout)\n",
        "    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list= list(backout.values())\n",
        "    #get gt\n",
        "    #vgt_class_batch, vgt_coord_batch = boxhead_network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "    #get feature vectors\n",
        "    # vfeature_vectors = maskhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    vfeature_vectors = maskhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    # print(\"feature_vectors = \", vfeature_vectors.detach().shape)\n",
        "    #pass through forward\n",
        "    maskout = maskhead_network_obj.forward(vfeature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN\n",
        "    #calc the loss\n",
        "    vloss= maskhead_network_obj.compute_loss_focal(maskout, labels, masks)\n",
        "\n",
        "    # vloss= maskhead_network_obj.compute_loss(vclass_prob, vbbox_reg, vgt_class_batch.to(device), vgt_coord_batch.to(device),effective_batch=150)\n",
        "    run_eval_loss += vloss.item()\n",
        "    # run_eval_loss_c += vloss_class.item()\n",
        "    # run_eval_loss_r += vloss_reg.item()\n",
        "    # del vclass_prob, vbbox_reg, vgt_class_batch, vgt_coord_batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  val_loss.append(run_eval_loss/len(full_loader))\n",
        "  # val_loss_c.append(run_eval_loss_c/len(full_loader))\n",
        "  # val_loss_r.append(run_eval_loss_r/len(full_loader))"
      ],
      "metadata": {
        "id": "n3pdovlEnE13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "oJZbR6yViOWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rvpCuvALu7s"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "\n",
        "ax[0].plot(train_loss)\n",
        "ax[0].set_title('Total Train Loss')\n",
        "ax[0].set_xlabel('epochs')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].grid()\n",
        "\n",
        "ax[1].plot(val_loss)\n",
        "ax[1].set_title('Total Validation Loss')\n",
        "ax[1].set_xlabel('epochs')\n",
        "ax[1].set_ylabel('loss')\n",
        "ax[1].grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19DhXLSy_aR"
      },
      "source": [
        "## post processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB9f09SgMaHu"
      },
      "outputs": [],
      "source": [
        "topK = 50\n",
        "boxhead_network_obj.eval()\n",
        "softmax = nn.Softmax()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (images, labels, masks, bboxes, indexes) in enumerate(train_loader):\n",
        "    backout = backbone(images)\n",
        "    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "    rpnout = rpn(im_lis, backout)\n",
        "    proposals=[proposal[0:topK,:].to(device) for proposal in rpnout[0]]\n",
        "    # proposals=[proposal[0:topK,:] for proposal in rpnout[0]]\n",
        "    fpn_feat_list= list(backout.values())\n",
        "    #get feature vectors\n",
        "    feature_vectors = boxhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "    class_prob, bbox_reg = boxhead_network_obj(feature_vectors.detach())      #TO ENSURE NO RETRAINING OF THE BACKBONE AND RPN\n",
        "    cls_soft = softmax(class_prob)    #do softmax now as we did not do it in forward function\n",
        "    #getmax confidence score and corresponding index\n",
        "    print(\"class_prob = \\n\", class_prob)\n",
        "    cls_with_softmax, max_indices = torch.max(cls_soft,dim=1)  \n",
        "    print(\"max_indices = \", max_indices)\n",
        "    for i in range(len(images)):\n",
        "        #display the image\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot()\n",
        "        ax.imshow(images[i].detach().cpu().numpy().transpose(1,2,0))\n",
        "        print(\"___________\")\n",
        "        cls_img, lbl = cls_with_softmax[i*topK:(i+1)*topK], max_indices[i*topK:(i+1)*topK]\n",
        "        #iterate over proposals\n",
        "        print(\"Proposals = \", len(proposals))\n",
        "        for j in range(len(proposals)):\n",
        "          #if label is 0, it is background so ignore\n",
        "          if(lbl[j]==0): \n",
        "            print(\"Background proposal \", j)\n",
        "            continue\n",
        "          #get single proposal and bounding box\n",
        "          edge_ind = (bbox_reg[j][((lbl[j]-1)*4).int() : ((lbl[j]-1)*4+4).int()]).cpu()\n",
        "          prop = (proposals[i][j]).cpu()\n",
        "          #decode the bounding box with corresponding proposal\n",
        "          tx, ty, tw, th = edge_ind[0], edge_ind[1], edge_ind[2], edge_ind[3]\n",
        "          xp, yp, wp, hp = (prop[0]+prop[2])/2, (prop[1]+prop[3])/2, prop[2]-prop[0], prop[3]-prop[1]\n",
        "          x, y, w, h = tx*wp+xp, ty*hp+yp, torch.exp(tw)*wp, torch.exp(th)*hp\n",
        "          x1, y1 = x-(w/2), y-(h/2)\n",
        "          print(x,y,w,h)\n",
        "          print(\"--------------------\")\n",
        "          #plot the top 20 predicted bbox regressions\n",
        "          if (lbl[j]==1): #----vehicle\n",
        "            rect = rec((x1, y1),w,h,fill=False,color='r',linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "          if (lbl[j]==2): #----human\n",
        "            rect = rec((x1, y1),w,h,fill=False,color='g',linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "          if (lbl[j]==3): #----animal\n",
        "            rect = rec((x1, y1),w,h,fill=False,color='b',linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "        plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "98yurxCAiXf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YemKqkSt8vpd"
      },
      "outputs": [],
      "source": [
        "def final_visualize(boxes_list, project_masks, clas, images):\n",
        "  for i in range(len(boxes_list)):    #number of examples to show\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot()\n",
        "    objs = boxes_list[i]\n",
        "    print(\"Objs = \", objs)\n",
        "    one_image = images[i].detach().cpu().numpy().transpose(1,2,0)\n",
        "    mask_vis = np.zeros_like(one_image, dtype=np.float64)\n",
        "    cat = clas[i]\n",
        "    final_mask = projected_masks[i]\n",
        "    ax.imshow(images[i].detach().cpu().numpy().transpose((1,2,0)))\n",
        "    for box in objs:\n",
        "      x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
        "      x1 = x1.cpu()\n",
        "      y1 = y1.cpu()\n",
        "      x2 = x2.cpu()\n",
        "      y2 = y2.cpu()\n",
        "      rect = rec((x1, y1), (x2 - x1), (y2 - y1), fill=False, color='r')\n",
        "      ax.add_patch(rect)\n",
        "      print(\"cat = \", cat.shape)\n",
        "      if(cat==0): #vehicle\n",
        "        mask_vis[:,:,0] = final_mask\n",
        "      if(cat==1): #person\n",
        "        mask_vis[:,:,1] = final_mask\n",
        "      if(cat==2): #animal\n",
        "        mask_vis[:,:,2] = final_mask\n",
        "      plt.imshow(mask_vis,alpha=0.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPxn1lmmy-f3"
      },
      "outputs": [],
      "source": [
        "path = \"/content/mask.ckpt\"\n",
        "checkpoint = torch.load(path)\n",
        "test_model = MaskHead().to(device)\n",
        "\n",
        "test_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "#in eval mode\n",
        "test_model.eval()\n",
        "for idx, (images, labels, masks, bboxes, indexes) in enumerate(test_loader):\n",
        "  #get features from box head\n",
        "  backout = backbone(images.to(device))\n",
        "  im_lis = ImageList(images, [(800, 1088)]*images.shape[0])\n",
        "  #pass thorugh rpn, get proposals and fpn_feat_list\n",
        "  rpnout = rpn(im_lis, backout)\n",
        "  proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]\n",
        "  fpn_feat_list = list(backout.values())\n",
        "  #get gt\n",
        "  #gt_class_batch, gt_coord_batch = boxhead_network_obj.create_batch_truth(bboxes, proposals, labels)\n",
        "  #get feature vectors\n",
        "  feature_vectors = maskhead_network_obj.find_feature_vectors(proposals, fpn_feat_list)\n",
        "  # print(\"feature_vectors = \", feature_vectors.detach().shape)\n",
        "  #pass through forward\n",
        "  maskout  = maskhead_network_obj.forward(feature_vectors.detach())  \n",
        "  # clas, regr = test_model.forward(features) #predict\n",
        "  projected_masks = test_model.postprocess_mask(maskout, bboxes, labels)  #post process\n",
        "  final_visualize(post_nms_predbox, projected_masks, labels , images)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "ZqZiiM76ZR5n"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}